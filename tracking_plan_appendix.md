## ğŸ“š ë¶€ë¡

### ë¶€ë¡ A: ì´ë²¤íŠ¸ & íŒŒë¼ë¯¸í„° ì„¤ê³„ ê°€ì´ë“œ

ì´ ë¶€ë¡ì€ ì‹¤ë¬´ì—ì„œ íŠ¸ë˜í‚¹ í”Œëœì„ ì„¤ê³„í•  ë•Œ í•„ìš”í•œ ê¸°ìˆ ì  ìƒì„¸ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤.

---

#### 1. íŒŒë¼ë¯¸í„°ì˜ ì¢…ë¥˜

íŒŒë¼ë¯¸í„°ëŠ” ì´ë²¤íŠ¸ ë°œìƒ ì‹œ í•¨ê»˜ ìˆ˜ì§‘í•˜ëŠ” ì¶”ê°€ ì •ë³´ì…ë‹ˆë‹¤.

**ê¸°ë³¸ ê°œë…:**
```
ğŸ“Œ ì´ë²¤íŠ¸ = ë™ì‚¬ (ë¬´ì—‡ì„ í–ˆëŠ”ê°€)
ğŸ“¦ íŒŒë¼ë¯¸í„° = ëª…ì‚¬ (ì–´ë–¤ ìƒíƒœì—ì„œ, ì–´ë–¤ ëŒ€ìƒìœ¼ë¡œ)
```

##### 1ï¸âƒ£ ì»¨í…ìŠ¤íŠ¸ íŒŒë¼ë¯¸í„° (Context Parameters)

ì´ë²¤íŠ¸ê°€ ë°œìƒí•œ í™˜ê²½ê³¼ ìƒí™© ì •ë³´

```javascript
{
  platform: "iOS",                    // í”Œë«í¼ (iOS, Android, Web)
  app_version: "2.3.1",              // ì•± ë²„ì „
  os_version: "iOS 17.2",            // OS ë²„ì „
  device_model: "iPhone 15 Pro",     // ê¸°ê¸° ëª¨ë¸
  screen_name: "product_detail",     // í˜„ì¬ í™”ë©´ëª…
  referrer: "search_result",         // ìœ ì… ê²½ë¡œ
  session_id: "sess_abc123",         // ì„¸ì…˜ ID
  network_type: "wifi"               // ë„¤íŠ¸ì›Œí¬ íƒ€ì…
}
```

**ì‹¤ë¬´ í™œìš©:**
- í”Œë«í¼ë³„ ì‚¬ìš©ì í–‰ë™ ì°¨ì´ ë¶„ì„
- ë²„ì „ë³„ ë²„ê·¸ ì¶”ì 
- ë””ë°”ì´ìŠ¤ë³„ ì„±ëŠ¥ ìµœì í™”

##### 2ï¸âƒ£ ë¹„ì¦ˆë‹ˆìŠ¤ íŒŒë¼ë¯¸í„° (Business Parameters)

ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ì— í•„ìš”í•œ í•µì‹¬ ì •ë³´

```javascript
{
  product_id: "PROD-12345",          // ìƒí’ˆ ê³ ìœ  ID
  product_name: "ë¬´ì„  ì´ì–´í°",        // ìƒí’ˆëª…
  category_depth1: "ì „ìì œí’ˆ",       // ëŒ€ë¶„ë¥˜
  category_depth2: "ì˜¤ë””ì˜¤",         // ì¤‘ë¶„ë¥˜
  category_depth3: "ì´ì–´í°",         // ì†Œë¶„ë¥˜
  price: 89000,                      // ê°€ê²© (ìˆ«ì)
  currency: "KRW",                   // í†µí™”
  discount_rate: 0.15,               // í• ì¸ìœ¨
  inventory_count: 42,               // ì¬ê³  ìˆ˜ëŸ‰
  seller_id: "seller_999",           // íŒë§¤ì ID
  brand: "BrandX"                    // ë¸Œëœë“œ
}
```

**ì‹¤ë¬´ í™œìš©:**
- ìƒí’ˆë³„ ì „í™˜ìœ¨ ë¶„ì„
- ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ë¶„ì„
- ì¬ê³  ë¶€ì¡± ì•Œë¦¼

##### 3ï¸âƒ£ ì‚¬ìš©ì íŒŒë¼ë¯¸í„° (User Parameters)

ì‚¬ìš©ì ì†ì„± ì •ë³´

```javascript
{
  user_id: "user_hashed_98765",      // í•´ì‹œëœ ì‚¬ìš©ì ID
  membership_tier: "premium",        // íšŒì› ë“±ê¸‰ (free, premium, vip)
  signup_date: "2024-03-15",        // ê°€ì…ì¼
  total_orders: 12,                  // ì´ ì£¼ë¬¸ íšŸìˆ˜
  lifetime_value: 1250000,          // ìƒì•  ê°€ì¹˜ (LTV)
  is_first_purchase: false,         // ì²« êµ¬ë§¤ ì—¬ë¶€
  user_segment: "high_value"        // ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸
}
```

**ì£¼ì˜ì‚¬í•­:**
- ê°œì¸ì •ë³´(ì´ë©”ì¼, ì „í™”ë²ˆí˜¸, ì´ë¦„)ëŠ” ì ˆëŒ€ ìˆ˜ì§‘ ê¸ˆì§€
- user_idëŠ” ë°˜ë“œì‹œ í•´ì‹±/ì•”í˜¸í™” ì²˜ë¦¬
- ê°€ë³€ì ì¸ ê°’(ë‚˜ì´, ê°€ì… í›„ ì¼ìˆ˜)ë³´ë‹¤ ë¶ˆë³€ê°’(ìƒë…„ì›”, ê°€ì…ì¼) ì‚¬ìš©

##### 4ï¸âƒ£ í–‰ë™ íŒŒë¼ë¯¸í„° (Behavior Parameters)

ì‚¬ìš©ì í–‰ë™ì˜ ì„¸ë¶€ ì •ë³´

```javascript
{
  click_position: 3,                 // ë¦¬ìŠ¤íŠ¸ì—ì„œ ëª‡ ë²ˆì§¸ í•­ëª© í´ë¦­
  scroll_depth_percent: 75,          // ìŠ¤í¬ë¡¤ ê¹Šì´ (%)
  time_on_page_seconds: 45,         // í˜ì´ì§€ ì²´ë¥˜ ì‹œê°„ (ì´ˆ)
  session_duration_seconds: 180,     // ì„¸ì…˜ ì§€ì† ì‹œê°„ (ì´ˆ)
  previous_screen: "home",          // ì´ì „ í™”ë©´
  search_query: "ë¬´ì„  ì´ì–´í°",       // ê²€ìƒ‰ì–´
  filter_applied: ["price_asc", "brand_apple"],  // ì ìš©í•œ í•„í„°
  video_watch_percent: 80           // ì˜ìƒ ì‹œì²­ë¥ 
}
```

**ì‹¤ë¬´ í™œìš©:**
- UX ê°œì„  (ì–´ë””ì„œ ì´íƒˆí•˜ëŠ”ê°€?)
- ì½˜í…ì¸  ìµœì í™” (ëª‡ %ê¹Œì§€ ë´¤ëŠ”ê°€?)
- ê²€ìƒ‰ ê¸°ëŠ¥ ê°œì„ 

---

#### 2. íŒŒë¼ë¯¸í„° ëª…ëª… ê·œì¹™

##### ê·œì¹™ 1: snake_case ì‚¬ìš©

```javascript
âœ… ì¢‹ì€ ì˜ˆ:
product_id
user_name
total_price
is_premium_member

âŒ ë‚˜ìœ ì˜ˆ:
productId          // camelCase
ProductID          // PascalCase
product-id         // kebab-case
PRODUCT_ID         // SCREAMING_SNAKE_CASE
```

**ì´ìœ :** ì¼ê´€ì„±, ê°€ë…ì„±, ë‹¤ì–‘í•œ í”Œë«í¼ í˜¸í™˜ì„±

##### ê·œì¹™ 2: ëª…í™•í•œ ë‹¨ìœ„ í‘œê¸°

```javascript
âœ… ì¢‹ì€ ì˜ˆ:
price: 89000               // ìˆ«ìë§Œ
currency: "KRW"            // í†µí™”ëŠ” ë³„ë„
duration_seconds: 180      // ë‹¨ìœ„ë¥¼ ì´ë¦„ì— í¬í•¨
weight_kg: 2.5            // ë‹¨ìœ„ë¥¼ ì´ë¦„ì— í¬í•¨
timestamp: "2025-01-15T10:30:00Z"  // ISO 8601 í˜•ì‹

âŒ ë‚˜ìœ ì˜ˆ:
price: "89,000ì›"         // ë¬¸ìì—´ + ë‹¨ìœ„
time: "3ë¶„"               // ë¶„ì„ ë¶ˆê°€ëŠ¥
date: "2025/1/15"         // ë¹„í‘œì¤€ í˜•ì‹
```

##### ê·œì¹™ 3: ì¼ê´€ëœ ë°ì´í„° íƒ€ì…

```javascript
âœ… ì¢‹ì€ ì˜ˆ:
is_premium: true           // Boolean
quantity: 3                // Integer
price: 89000               // Integer (ì›í™”ëŠ” ì†Œìˆ˜ì  ì—†ìŒ)
discount_rate: 0.15        // Float (15%)
product_id: "PROD-12345"   // String

âŒ ë‚˜ìœ ì˜ˆ:
is_premium: "Y"            // String (Booleanì´ì–´ì•¼ í•¨)
quantity: "3ê°œ"            // String (Integerì—¬ì•¼ í•¨)
price: "89000"             // String (ìˆ«ìì—¬ì•¼ í•¨)
discount_rate: "15%"       // String (Floatì´ì–´ì•¼ í•¨)
```

##### ê·œì¹™ 4: ê³„ì¸µ êµ¬ì¡° í‘œí˜„

```javascript
âœ… ì¢‹ì€ ì˜ˆ - ë¶„ë¦¬ëœ í•„ë“œ:
category_depth1: "íŒ¨ì…˜"
category_depth2: "ë‚¨ì„±ì˜ë¥˜"
category_depth3: "ì•„ìš°í„°"

address_city: "ì„œìš¸"
address_district: "ê°•ë‚¨êµ¬"
address_detail: "í…Œí—¤ë€ë¡œ"

âŒ ë‚˜ìœ ì˜ˆ - ê²°í•©ëœ ê°’:
category: "íŒ¨ì…˜ > ë‚¨ì„±ì˜ë¥˜ > ì•„ìš°í„°"    // ë¶„ì„ ì–´ë ¤ì›€
address: "ì„œìš¸ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ"         // íŒŒì‹± í•„ìš”
```

**ì´ìœ :**
- í•„í„°ë§ ìš©ì´ (ì˜ˆ: "íŒ¨ì…˜" ì¹´í…Œê³ ë¦¬ ì „ì²´ ë¶„ì„)
- ì§‘ê³„ ìš©ì´ (ì˜ˆ: ì§€ì—­ë³„ ë§¤ì¶œ)
- ë°ì´í„° ì •í•©ì„±

##### ê·œì¹™ 5: ì•½ì–´ ì‚¬ìš© ìµœì†Œí™”

```javascript
âœ… ì¢‹ì€ ì˜ˆ:
product_id
user_id
referrer_url
category

âŒ ë‚˜ìœ ì˜ˆ:
prod_id
usr_id
ref_url
cat
```

**ì˜ˆì™¸:** ì—…ê³„ í‘œì¤€ ì•½ì–´ëŠ” ì‚¬ìš© ê°€ëŠ¥
- `url` (Uniform Resource Locator)
- `id` (Identifier)
- `utm` (Urchin Tracking Module)

---

#### 3. ì‹¤ì „ ì˜ˆì‹œ: ì´ì»¤ë¨¸ìŠ¤ êµ¬ë§¤ í”Œë¡œìš°

##### ì´ë²¤íŠ¸ 1: ìƒí’ˆ ì¡°íšŒ

```javascript
{
  event_name: "product_view",
  timestamp: "2025-01-15T10:00:00Z",

  // ìƒí’ˆ ì •ë³´
  product_id: "PROD-12345",
  product_name: "ë¬´ì„  ì´ì–´í° í”„ë¦¬ë¯¸ì—„",
  category_depth1: "ì „ìì œí’ˆ",
  category_depth2: "ì˜¤ë””ì˜¤",
  category_depth3: "ì´ì–´í°",
  price: 89000,
  currency: "KRW",
  discount_rate: 0.15,
  final_price: 75650,
  brand: "BrandX",

  // ì‚¬ìš©ì ì •ë³´
  user_id: "user_hashed_abc123",
  membership_tier: "premium",

  // ì»¨í…ìŠ¤íŠ¸
  platform: "iOS",
  device_model: "iPhone 15",
  screen_name: "product_detail",
  referrer: "search_result",
  search_query: "ë¬´ì„  ì´ì–´í°",

  // í–‰ë™
  view_source: "search",      // ì–´ë–»ê²Œ ë„ë‹¬í–ˆëŠ”ê°€
  position_in_list: 3        // ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ëª‡ ë²ˆì§¸
}
```

##### ì´ë²¤íŠ¸ 2: ì¥ë°”êµ¬ë‹ˆ ì¶”ê°€

```javascript
{
  event_name: "add_to_cart",
  timestamp: "2025-01-15T10:02:30Z",

  // ìƒí’ˆ ì •ë³´
  product_id: "PROD-12345",
  quantity: 1,
  price: 89000,
  final_price: 75650,

  // ì¥ë°”êµ¬ë‹ˆ ìƒíƒœ
  cart_total_items: 3,        // ì¶”ê°€ í›„ ì¥ë°”êµ¬ë‹ˆ ì´ ìƒí’ˆ ìˆ˜
  cart_total_value: 215000,   // ì¶”ê°€ í›„ ì¥ë°”êµ¬ë‹ˆ ì´ ê¸ˆì•¡

  // ì‚¬ìš©ì
  user_id: "user_hashed_abc123",

  // ì»¨í…ìŠ¤íŠ¸
  platform: "iOS",
  screen_name: "product_detail",

  // í–‰ë™
  time_on_page_seconds: 150   // ìƒí’ˆ í˜ì´ì§€ ì²´ë¥˜ ì‹œê°„
}
```

##### ì´ë²¤íŠ¸ 3: ê²°ì œ ì‹œì‘

```javascript
{
  event_name: "checkout_start",
  timestamp: "2025-01-15T10:05:00Z",

  // ì£¼ë¬¸ ì •ë³´
  cart_total_items: 3,
  cart_total_value: 215000,
  shipping_fee: 3000,
  discount_amount: 10000,
  final_amount: 208000,

  // ì¿ í°
  coupon_applied: true,
  coupon_code: "WELCOME2025",
  coupon_discount: 10000,

  // ì‚¬ìš©ì
  user_id: "user_hashed_abc123",
  membership_tier: "premium",

  // ì»¨í…ìŠ¤íŠ¸
  platform: "iOS"
}
```

##### ì´ë²¤íŠ¸ 4: ê²°ì œ ì™„ë£Œ

```javascript
{
  event_name: "purchase_complete",
  timestamp: "2025-01-15T10:07:45Z",

  // ì£¼ë¬¸ ì •ë³´
  order_id: "ORD-2025-001234",
  revenue: 208000,             // ì‹¤ì œ ê²°ì œ ê¸ˆì•¡
  shipping_fee: 3000,
  tax: 0,

  // ê²°ì œ ì •ë³´
  payment_method: "card",      // card, bank_transfer, mobile
  payment_provider: "kakaopay",
  installment_months: 0,       // í• ë¶€ ê°œì›” (0 = ì¼ì‹œë¶ˆ)

  // ë°°ì†¡ ì •ë³´
  shipping_method: "express",  // standard, express, pickup
  estimated_delivery_date: "2025-01-17",

  // ìƒí’ˆ ëª©ë¡ (ë°°ì—´)
  products: [
    {
      product_id: "PROD-12345",
      quantity: 1,
      price: 75650
    },
    // ... ê¸°íƒ€ ìƒí’ˆ
  ],

  // ì‚¬ìš©ì
  user_id: "user_hashed_abc123",
  is_first_purchase: false,

  // ì»¨í…ìŠ¤íŠ¸
  platform: "iOS",

  // í¼ë„ ë¶„ì„ìš©
  time_from_view_to_purchase_seconds: 465  // ìƒí’ˆ ì¡°íšŒë¶€í„° êµ¬ë§¤ê¹Œì§€ ì‹œê°„
}
```

---

#### 4. ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜ì™€ í•´ê²°ì±…

##### ì‹¤ìˆ˜ 1: ê°œì¸ì •ë³´ ìˆ˜ì§‘

```javascript
âŒ ì ˆëŒ€ ê¸ˆì§€:
{
  email: "user@example.com",
  phone: "010-1234-5678",
  name: "í™ê¸¸ë™",
  address: "ì„œìš¸ì‹œ ê°•ë‚¨êµ¬...",
  birth_date: "1990-05-15",
  card_number: "1234-****-****-5678"
}

âœ… ì˜¬ë°”ë¥¸ ë°©ë²•:
{
  user_id: "user_hashed_abc123",  // í•´ì‹±ëœ ID
  age_group: "30s",                // ì—°ë ¹ëŒ€
  region: "seoul",                 // ì§€ì—­ (êµ¬ì²´ì  ì£¼ì†Œ X)
  payment_method: "card"           // ì¹´ë“œë²ˆí˜¸ X, ê²°ì œìˆ˜ë‹¨ë§Œ
}
```

**ë²•ì  ê·¼ê±°:** ê°œì¸ì •ë³´ë³´í˜¸ë²•, GDPR

##### ì‹¤ìˆ˜ 2: ë¶ˆí•„ìš”í•œ ì¤‘ë³µ

```javascript
âŒ ë‚˜ìœ ì˜ˆ:
{
  event_name: "product_click",
  product_id: "12345",
  product_name: "ë¬´ì„  ì´ì–´í°",
  product_price: 89000,
  product_category: "ì „ìì œí’ˆ",
  product_brand: "BrandX"
}

âœ… ì¢‹ì€ ì˜ˆ:
{
  event_name: "product_click",
  id: "12345",
  name: "ë¬´ì„  ì´ì–´í°",
  price: 89000,
  category: "ì „ìì œí’ˆ",
  brand: "BrandX"
}
```

**ì´ìœ :** `product_` ì ‘ë‘ì‚¬ëŠ” ì´ë²¤íŠ¸ëª…ì—ì„œ ì´ë¯¸ ëª…í™•í•¨

##### ì‹¤ìˆ˜ 3: ê°€ë³€ì ì¸ ê°’ ì €ì¥

```javascript
âŒ ë‚˜ìœ ì˜ˆ:
{
  user_age: 35,                    // 1ë…„ í›„ ì˜ëª»ëœ ê°’
  days_since_signup: 120,          // ë§¤ì¼ ë³€í•¨
  hours_since_last_login: 5        // ì‹œê°„ë§ˆë‹¤ ë³€í•¨
}

âœ… ì¢‹ì€ ì˜ˆ:
{
  birth_year: 1990,                // ë¶ˆë³€
  signup_date: "2024-09-01",       // ë¶ˆë³€
  last_login_timestamp: "2025-01-15T05:00:00Z"  // ì ˆëŒ€ ì‹œê°„
}
```

**ì´ìœ :** ë¶„ì„ ì‹œì ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§€ë©´ ì•ˆ ë¨

##### ì‹¤ìˆ˜ 4: ë¬¸ìì—´ì— ìˆ«ì ì„ê¸°

```javascript
âŒ ë‚˜ìœ ì˜ˆ:
{
  price: "89,000ì›",
  quantity: "3ê°œ",
  discount: "15%",
  duration: "2ì‹œê°„ 30ë¶„"
}

âœ… ì¢‹ì€ ì˜ˆ:
{
  price: 89000,
  currency: "KRW",
  quantity: 3,
  discount_rate: 0.15,
  duration_minutes: 150
}
```

**ì´ìœ :** ì§‘ê³„, í•„í„°ë§, ì •ë ¬ ë¶ˆê°€ëŠ¥

##### ì‹¤ìˆ˜ 5: ë¶ˆëª…í™•í•œ Boolean í‘œí˜„

```javascript
âŒ ë‚˜ìœ ì˜ˆ:
{
  is_premium: "Y",
  has_discount: "true",
  stock_available: 1,
  newsletter_subscribed: "yes"
}

âœ… ì¢‹ì€ ì˜ˆ:
{
  is_premium: true,
  has_discount: true,
  stock_available: true,
  newsletter_subscribed: true
}
```

---

#### 5. ê³ ê¸‰ íŒ¨í„´

##### íŒ¨í„´ 1: ë°°ì—´ íŒŒë¼ë¯¸í„°

ìƒí’ˆ ëª©ë¡, í•„í„° ëª©ë¡ ë“±

```javascript
{
  event_name: "purchase_complete",
  order_id: "ORD-001",

  // ìƒí’ˆ ë°°ì—´
  products: [
    {
      id: "PROD-123",
      name: "ë¬´ì„  ì´ì–´í°",
      quantity: 1,
      price: 89000
    },
    {
      id: "PROD-456",
      name: "ìŠ¤ë§ˆíŠ¸ì›Œì¹˜",
      quantity: 2,
      price: 250000
    }
  ],

  // í•„í„° ë°°ì—´
  applied_filters: ["price_asc", "brand_apple", "rating_4plus"]
}
```

**ì£¼ì˜:** ë°°ì—´ì€ ë¶„ì„ì´ ë³µì¡í•˜ë¯€ë¡œ ê¼­ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©

##### íŒ¨í„´ 2: ì¡°ê±´ë¶€ íŒŒë¼ë¯¸í„°

íŠ¹ì • ì¡°ê±´ì—ì„œë§Œ ìˆ˜ì§‘

```javascript
// ê²€ìƒ‰ ì´ë²¤íŠ¸
{
  event_name: "search",
  query: "ë¬´ì„  ì´ì–´í°",
  results_count: 42,

  // ê²€ìƒ‰ ê²°ê³¼ê°€ 0ì¼ ë•Œë§Œ
  is_zero_results: true,           // results_countê°€ 0ì¸ ê²½ìš°ì—ë§Œ
  suggested_query: "ë¸”ë£¨íˆ¬ìŠ¤ ì´ì–´í°"  // 0 resultsì¼ ë•Œ ì œì•ˆí•œ ê²€ìƒ‰ì–´
}

// ê²°ì œ ì´ë²¤íŠ¸
{
  event_name: "purchase_complete",
  order_id: "ORD-001",

  // ì¿ í° ì‚¬ìš© ì‹œì—ë§Œ
  coupon_applied: true,
  coupon_code: "WELCOME2025",      // coupon_appliedê°€ trueì¼ ë•Œë§Œ
  coupon_discount: 10000           // coupon_appliedê°€ trueì¼ ë•Œë§Œ
}
```

##### íŒ¨í„´ 3: í”Œë«í¼ë³„ íŒŒë¼ë¯¸í„°

```javascript
// iOSì—ì„œë§Œ
{
  event_name: "app_open",
  platform: "iOS",
  idfa: "hashed_idfa_value",       // iOSë§Œ
  att_status: "authorized"         // iOS 14+ ì¶”ì  ê¶Œí•œ
}

// Androidì—ì„œë§Œ
{
  event_name: "app_open",
  platform: "Android",
  gaid: "hashed_gaid_value",       // Androidë§Œ
  play_install_referrer: "..."     // Androidë§Œ
}

// Webì—ì„œë§Œ
{
  event_name: "page_view",
  platform: "Web",
  url: "https://example.com/products/123",
  referrer_url: "https://google.com",
  utm_source: "google",
  utm_medium: "cpc"
}
```

---

ì´ê²ƒìœ¼ë¡œ ë¶€ë¡ Aë¥¼ ë§ˆì¹©ë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ê³„í•  ë•Œ ì´ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

---

### ë¶€ë¡ B: ì‹¤ë¬´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì´í•´í•˜ê¸°

ì´ ë¶€ë¡ì€ íŠ¸ë˜í‚¹ í”Œëœìœ¼ë¡œ ì„¤ê³„í•œ ì´ë²¤íŠ¸ê°€ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ìˆ˜ì§‘ë˜ê³ , ì €ì¥ë˜ê³ , ë¶„ì„ë˜ëŠ”ì§€ ì „ì²´ íë¦„ì„ ë‹¤ë£¹ë‹ˆë‹¤.

---

#### 1. ë°ì´í„° ìƒì„±ë¶€í„° ë¶„ì„ê¹Œì§€ ì „ì²´ ì—¬ì •

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ìƒì„±    â”‚ --> â”‚  2. ì „ì†¡    â”‚ --> â”‚  3. ì €ì¥    â”‚ --> â”‚  4. ë¶„ì„    â”‚
â”‚  (ì•±/ì›¹)    â”‚     â”‚  (SDK)      â”‚     â”‚ (ë°ì´í„°ë² ì´ìŠ¤)â”‚     â”‚ (SQL/Python)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### ë‹¨ê³„ 1: ë°ì´í„° ìƒì„± (ì•±/ì›¹)

ì‚¬ìš©ìê°€ ì•±ì´ë‚˜ ì›¹ì‚¬ì´íŠ¸ì—ì„œ í–‰ë™ì„ í•˜ë©´ ì´ë²¤íŠ¸ê°€ ë°œìƒí•©ë‹ˆë‹¤.

```javascript
// ì‚¬ìš©ìê°€ "êµ¬ë§¤í•˜ê¸°" ë²„íŠ¼ì„ í´ë¦­
button.onClick(() => {
  // ì´ë²¤íŠ¸ ì „ì†¡
  analytics.track('purchase_button_click', {
    product_id: 'PROD-12345',
    price: 89000,
    platform: 'iOS'
  });
});
```

##### ë‹¨ê³„ 2: ë°ì´í„° ì „ì†¡ (SDK)

SDK(Software Development Kit)ê°€ ì´ë²¤íŠ¸ë¥¼ ì„œë²„ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.

**ì£¼ìš” SDK:**
- Google Analytics (GA4)
- Amplitude
- Mixpanel
- Segment (ì—¬ëŸ¬ ë¶„ì„ ë„êµ¬ë¥¼ í†µí•©)
- Firebase Analytics
- ìì²´ ê°œë°œ SDK

**ì „ì†¡ ë°©ì‹:**
- HTTP POST ìš”ì²­
- ë°°ì¹˜ ì²˜ë¦¬ (ì—¬ëŸ¬ ì´ë²¤íŠ¸ë¥¼ ëª¨ì•„ì„œ í•œ ë²ˆì— ì „ì†¡)
- ì‹¤ì‹œê°„ ì „ì†¡ vs ì§€ì—° ì „ì†¡ (ë„¤íŠ¸ì›Œí¬ ìƒí™©ì— ë”°ë¼)

##### ë‹¨ê³„ 3: ë°ì´í„° ì €ì¥

ìˆ˜ì§‘ëœ ì´ë²¤íŠ¸ëŠ” ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì €ì¥ë©ë‹ˆë‹¤.

**ì €ì¥ ìœ„ì¹˜:**
- ë¡œê·¸ íŒŒì¼ (ì„œë²„ íŒŒì¼ ì‹œìŠ¤í…œ)
- ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ (MySQL, PostgreSQL)
- NoSQL ë°ì´í„°ë² ì´ìŠ¤ (MongoDB)
- ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ (BigQuery, Redshift, Snowflake)
- ë¶„ì„ í”Œë«í¼ (Amplitude, Mixpanel)

##### ë‹¨ê³„ 4: ë°ì´í„° ë¶„ì„

ì €ì¥ëœ ë°ì´í„°ë¥¼ SQL, Python ë“±ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

```sql
-- BigQueryì—ì„œ ì¼ë³„ ë§¤ì¶œ ì§‘ê³„
SELECT
  DATE(timestamp) as date,
  COUNT(*) as purchase_count,
  SUM(revenue) as total_revenue
FROM events
WHERE event_name = 'purchase_complete'
GROUP BY date
ORDER BY date DESC;
```

---

#### 2. ì‹¤ë¬´ì—ì„œ ë§Œë‚˜ëŠ” ë°ì´í„° í˜•ì‹

##### í˜•ì‹ 1: CSV (Comma-Separated Values)

**ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?**
- ì†Œê·œëª¨ ë°ì´í„° ë¶„ì„
- BI ë„êµ¬ì—ì„œ ë‹¤ìš´ë¡œë“œ
- Excelë¡œ ê°„ë‹¨íˆ í™•ì¸í•  ë•Œ
- ë°ì´í„° ê³µìœ  (ì´ë©”ì¼, ìŠ¬ë™)

**ì‹¤ì œ íŒŒì¼ ì˜ˆì‹œ:**

`events.csv`
```csv
event_name,user_id,product_id,price,timestamp
product_view,user_001,PROD-123,89000,2025-01-15T10:00:00Z
add_to_cart,user_001,PROD-123,89000,2025-01-15T10:02:30Z
purchase_complete,user_001,PROD-123,89000,2025-01-15T10:05:45Z
product_view,user_002,PROD-456,150000,2025-01-15T10:01:00Z
```

**ì¥ì :**
- ì‚¬ëŒì´ ì½ê¸° ì‰¬ì›€
- Excel, Google Sheetsì—ì„œ ë°”ë¡œ ì—´ë¦¼
- ëª¨ë“  ë„êµ¬ì—ì„œ ì§€ì›

**ë‹¨ì :**
- í° íŒŒì¼ í¬ê¸° (ì••ì¶• ì•ˆ ë¨)
- ë³µì¡í•œ êµ¬ì¡° í‘œí˜„ ì–´ë ¤ì›€ (ì¤‘ì²© ê°ì²´, ë°°ì—´)
- ë°ì´í„° íƒ€ì… ì •ë³´ ì—†ìŒ (ëª¨ë“  ê²ƒì´ ë¬¸ìì—´)
- ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ëŠë¦¼

**ì‹¤ë¬´ ì‚¬ìš© ì˜ˆ:**
```
ì‚¬ìš© ì‚¬ë¡€: Amplitudeì—ì„œ "ì§€ë‚œ 7ì¼ êµ¬ë§¤ ë°ì´í„°" CSV ë‹¤ìš´ë¡œë“œ
â†“
Excelì—ì„œ ì—´ì–´ì„œ ê°„ë‹¨í•œ í”¼ë²— í…Œì´ë¸” ìƒì„±
â†“
ì£¼ê°„ ë¦¬í¬íŠ¸ì— ì²¨ë¶€
```

##### í˜•ì‹ 2: JSON Lines (.jsonl)

**ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?**
- ë¡œê·¸ íŒŒì¼ ì €ì¥
- ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°
- ì„œë²„ â†’ ì„œë²„ ë°ì´í„° ì „ì†¡
- ë³µì¡í•œ ë°ì´í„° êµ¬ì¡° (ì¤‘ì²© ê°ì²´, ë°°ì—´)

**ì‹¤ì œ íŒŒì¼ ì˜ˆì‹œ:**

`events.jsonl`
```json
{"event_name":"product_view","user_id":"user_001","product_id":"PROD-123","price":89000,"timestamp":"2025-01-15T10:00:00Z","metadata":{"platform":"iOS","device":"iPhone 15"}}
{"event_name":"add_to_cart","user_id":"user_001","product_id":"PROD-123","quantity":1,"price":89000,"timestamp":"2025-01-15T10:02:30Z"}
{"event_name":"purchase_complete","user_id":"user_001","order_id":"ORD-001","revenue":89000,"products":[{"id":"PROD-123","quantity":1,"price":89000}],"timestamp":"2025-01-15T10:05:45Z"}
```

**íŠ¹ì§•:**
- í•œ ì¤„ì— í•˜ë‚˜ì˜ ì´ë²¤íŠ¸ (JSON ê°ì²´)
- ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ê°€ëŠ¥ (ì¤„ ë‹¨ìœ„ë¡œ ì½ê¸°)
- ë³µì¡í•œ êµ¬ì¡° í‘œí˜„ ê°€ëŠ¥

**ì¥ì :**
- ë³µì¡í•œ ë°ì´í„° êµ¬ì¡° ì§€ì› (ë°°ì—´, ì¤‘ì²© ê°ì²´)
- ë°ì´í„° íƒ€ì… ë³´ì¡´ (ìˆ«ì, ë¶ˆë¦°, null)
- ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
- ì‚¬ëŒì´ ì½ê¸° ì‰¬ì›€ (ë””ë²„ê¹… ìš©ì´)

**ë‹¨ì :**
- CSVë³´ë‹¤ íŒŒì¼ í¬ê¸° í¼
- ì²˜ë¦¬ ì†ë„ ëŠë¦¼ (íŒŒì‹± í•„ìš”)

**ì‹¤ë¬´ ì‚¬ìš© ì˜ˆ:**
```
ì‚¬ìš© ì‚¬ë¡€: ì„œë²„ ë¡œê·¸ íŒŒì¼
/var/log/events/2025-01-15.jsonl  (í•˜ë£¨ì¹˜ ì´ë²¤íŠ¸)
â†“
Kafkaë¡œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
â†“
BigQueryë¡œ ì ì¬
```

##### í˜•ì‹ 3: Parquet

**ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?**
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì €ì¥
- í´ë¼ìš°ë“œ ì €ì¥ì†Œ (S3, GCS)
- ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ (BigQuery, Redshift)
- ë¹ ë¥¸ ì§‘ê³„ ë¶„ì„

**íŠ¹ì§•:**
- ì»¬ëŸ¼ ê¸°ë°˜ ì €ì¥ (Column-oriented)
- ì••ì¶•ë¥  ë†’ìŒ (CSV ëŒ€ë¹„ 10~100ë°° ì‘ìŒ)
- ë°ì´í„° íƒ€ì… ë³´ì¡´
- ì‚¬ëŒì´ ì§ì ‘ ì½ì„ ìˆ˜ ì—†ìŒ (ë°”ì´ë„ˆë¦¬ í˜•ì‹)

**ì¥ì :**
- ë§¤ìš° ì‘ì€ íŒŒì¼ í¬ê¸° (ì••ì¶•)
- ë¹ ë¥¸ ì§‘ê³„ (ì»¬ëŸ¼ ë‹¨ìœ„ ì½ê¸°)
- ë°ì´í„° íƒ€ì…, ìŠ¤í‚¤ë§ˆ í¬í•¨
- í´ë¼ìš°ë“œ ìµœì í™”

**ë‹¨ì :**
- ë°”ì´ë„ˆë¦¬ í˜•ì‹ (ì‚¬ëŒì´ ì½ì„ ìˆ˜ ì—†ìŒ)
- íŠ¹ìˆ˜ ë„êµ¬ í•„ìš” (Pandas, Spark, BigQuery)
- í–‰ ì¶”ê°€ ì–´ë ¤ì›€ (ì½ê¸° ì „ìš©ì— ê°€ê¹Œì›€)

**ì‹¤ë¬´ ì‚¬ìš© ì˜ˆ:**
```
ì‚¬ìš© ì‚¬ë¡€: S3ì— ì›”ë³„ ì´ë²¤íŠ¸ ì €ì¥
s3://bucket/events/year=2025/month=01/events.parquet
â†“
Athena ë˜ëŠ” BigQueryë¡œ ì¿¼ë¦¬
â†“
Tableauë¡œ ì‹œê°í™”
```

**íŒŒì¼ í¬ê¸° ë¹„êµ (1ë°±ë§Œ ê±´ ì´ë²¤íŠ¸):**
```
CSV:       1.2 GB
JSON:      1.8 GB
JSONL:     1.5 GB
Parquet:   120 MB  (10ë°° ì‘ìŒ)
```

##### í˜•ì‹ 4: Google Analytics 4 (GA4) BigQuery Export

**ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?**
- GA4ë¥¼ ì‚¬ìš©í•˜ëŠ” ì›¹/ì•±
- BigQueryë¡œ ì›ë³¸ ë°ì´í„° ë¶„ì„
- ì»¤ìŠ¤í…€ ë¦¬í¬íŠ¸ í•„ìš”

**ì‹¤ì œ ë°ì´í„° êµ¬ì¡°:**

BigQuery í…Œì´ë¸”: `analytics_123456789.events_20250115`

```sql
SELECT
  event_date,              -- 20250115 (YYYYMMDD)
  event_timestamp,         -- 1736928000000000 (ë§ˆì´í¬ë¡œì´ˆ)
  event_name,              -- 'purchase'
  user_pseudo_id,          -- 'abc123...'

  -- ì´ë²¤íŠ¸ íŒŒë¼ë¯¸í„° (ë°°ì—´)
  (SELECT value.string_value
   FROM UNNEST(event_params)
   WHERE key = 'product_id') as product_id,

  (SELECT value.int_value
   FROM UNNEST(event_params)
   WHERE key = 'price') as price

FROM `analytics_123456789.events_*`
WHERE _TABLE_SUFFIX = '20250115'
LIMIT 10;
```

**íŠ¹ì§•:**
- ë‚ ì§œë³„ íŒŒí‹°ì…˜ í…Œì´ë¸” (`events_20250115`, `events_20250116`, ...)
- ì¤‘ì²© êµ¬ì¡° (event_paramsëŠ” ë°°ì—´)
- UNNEST í•„ìš” (íŒŒë¼ë¯¸í„° ì¶”ì¶œ)

**ì¥ì :**
- GA4ì˜ ëª¨ë“  ì›ë³¸ ë°ì´í„° ì ‘ê·¼
- BigQueryì˜ ê°•ë ¥í•œ ë¶„ì„ ê¸°ëŠ¥
- ë‹¤ë¥¸ ë°ì´í„°ì™€ JOIN ê°€ëŠ¥

**ë‹¨ì :**
- ë³µì¡í•œ ì¿¼ë¦¬ êµ¬ì¡° (UNNEST)
- í•™ìŠµ ê³¡ì„  ë†’ìŒ
- BigQuery ë¹„ìš© ë°œìƒ

##### í˜•ì‹ 5: Amplitude Export

**ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?**
- Amplitude ì‚¬ìš© ì¤‘
- ì›ë³¸ ë°ì´í„° ì¶”ì¶œ í•„ìš”
- ì»¤ìŠ¤í…€ ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹

**ì‹¤ì œ ë°ì´í„° í˜•ì‹:**

Export API ì‘ë‹µ (JSON):
```json
{
  "data": [
    {
      "event_type": "purchase_complete",
      "user_id": "user_abc123",
      "event_time": "2025-01-15 10:05:45.123",
      "event_properties": {
        "product_id": "PROD-123",
        "price": 89000,
        "currency": "KRW"
      },
      "user_properties": {
        "membership_tier": "premium",
        "total_orders": 12
      },
      "platform": "iOS",
      "device_model": "iPhone 15"
    }
  ]
}
```

**íŠ¹ì§•:**
- REST APIë¡œ ë°ì´í„° ì¶”ì¶œ
- JSON í˜•ì‹
- í˜ì´ì§€ë„¤ì´ì…˜ (ëŒ€ìš©ëŸ‰ ë°ì´í„°)

---

#### 3. ë°ì´í„° í˜•ì‹ ë¹„êµí‘œ

| í˜•ì‹ | ì‚¬ìš© ì‹œê¸° | ì¥ì  | ë‹¨ì  | íŒŒì¼ í¬ê¸° |
|------|----------|------|------|-----------|
| **CSV** | ì†Œê·œëª¨, ê³µìœ , Excel ë¶„ì„ | ë²”ìš©ì„±, ê°€ë…ì„± | í° í¬ê¸°, íƒ€ì… ì—†ìŒ | â­â­â­â­â­ |
| **JSONL** | ë¡œê·¸, ìŠ¤íŠ¸ë¦¬ë°, ë³µì¡í•œ êµ¬ì¡° | êµ¬ì¡° í‘œí˜„, íƒ€ì… ë³´ì¡´ | í° í¬ê¸°, ëŠë¦° ì²˜ë¦¬ | â­â­â­â­ |
| **Parquet** | ëŒ€ìš©ëŸ‰, í´ë¼ìš°ë“œ, ë¹ ë¥¸ ì§‘ê³„ | ì‘ì€ í¬ê¸°, ë¹ ë¥¸ ì†ë„ | ë°”ì´ë„ˆë¦¬, ë„êµ¬ í•„ìš” | â­ |
| **GA4** | GA4 ì‚¬ìš©ì, BigQuery ë¶„ì„ | ì›ë³¸ ë°ì´í„°, ê°•ë ¥í•œ ë¶„ì„ | ë³µì¡í•œ ì¿¼ë¦¬, ë¹„ìš© | N/A |
| **Amplitude** | Amplitude ì‚¬ìš©ì, API ì¶”ì¶œ | êµ¬ì¡°í™”ëœ ë°ì´í„° | API ì œí•œ, í˜ì´ì§€ë„¤ì´ì…˜ | N/A |

---

#### 4. ì‹¤ë¬´ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë°ì´í„° íë¦„

##### ì‹œë‚˜ë¦¬ì˜¤ 1: ìŠ¤íƒ€íŠ¸ì—… (ì›” 10ë§Œ ì´ë²¤íŠ¸)

```
ì•±/ì›¹
  â†“
Google Analytics 4 (ë¬´ë£Œ)
  â†“
BigQuery Export (ë¬´ë£Œ í‹°ì–´)
  â†“
Looker Studio (ë¬´ë£Œ)
```

**ë°ì´í„° í˜•ì‹:**
- GA4 ë‚´ë¶€: GA4 ì „ìš© í˜•ì‹
- BigQuery: Parquet (ìë™)
- ë¶„ì„: SQL

**ë¹„ìš©:** ê±°ì˜ ë¬´ë£Œ (BigQuery ë¬´ë£Œ í‹°ì–´ 1TB/ì›”)

##### ì‹œë‚˜ë¦¬ì˜¤ 2: ì¤‘ì†Œê¸°ì—… (ì›” 1000ë§Œ ì´ë²¤íŠ¸)

```
ì•±/ì›¹
  â†“
Segment (í†µí•© SDK)
  â†“
â”œâ”€ Amplitude (í–‰ë™ ë¶„ì„)
â”œâ”€ BigQuery (ì›ë³¸ ë°ì´í„°)
â””â”€ S3 (ë°±ì—…)
  â†“
Tableau / Looker
```

**ë°ì´í„° í˜•ì‹:**
- Segment â†’ Amplitude: JSON (API)
- Segment â†’ BigQuery: Parquet
- BigQuery â†’ S3: Parquet
- ë°±ì—…: JSONL (S3)

**ë¹„ìš©:** ì›” ìˆ˜ë°±ë§Œ ì› (Segment, Amplitude, BigQuery)

##### ì‹œë‚˜ë¦¬ì˜¤ 3: ëŒ€ê¸°ì—… (ì›” 10ì–µ ì´ë²¤íŠ¸)

```
ì•±/ì›¹
  â†“
ìì²´ SDK
  â†“
Kafka (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
  â†“
â”œâ”€ BigQuery (ì‹¤ì‹œê°„ ë¶„ì„)
â”œâ”€ Redshift (ë°°ì¹˜ ë¶„ì„)
â””â”€ S3 (ì›ë³¸ ì €ì¥)
  â†“
Looker / Tableau / ìì²´ ëŒ€ì‹œë³´ë“œ
```

**ë°ì´í„° í˜•ì‹:**
- Kafka: JSONL (ìŠ¤íŠ¸ë¦¬ë°)
- S3: Parquet (íŒŒí‹°ì…˜)
- BigQuery: Parquet (ìë™)
- ë°±ì—…: Parquet (ì••ì¶•)

**ë¹„ìš©:** ì›” ìˆ˜ì²œë§Œ ì› (ì¸í”„ë¼, ì—”ì§€ë‹ˆì–´ë§)

---

#### 5. ë°ì´í„° í™•ì¸ ë°©ë²•

##### ë°©ë²• 1: ë¡œê·¸ íŒŒì¼ ì§ì ‘ í™•ì¸ (ê°œë°œì)

```bash
# ì„œë²„ì— SSH ì ‘ì†
ssh user@server

# ì˜¤ëŠ˜ ë¡œê·¸ í™•ì¸
tail -f /var/log/events/2025-01-15.jsonl

# íŠ¹ì • ì´ë²¤íŠ¸ ê²€ìƒ‰
grep "purchase_complete" /var/log/events/2025-01-15.jsonl | head -5

# ì´ë²¤íŠ¸ ê°œìˆ˜ ì„¸ê¸°
wc -l /var/log/events/2025-01-15.jsonl
```

##### ë°©ë²• 2: ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ (ë°ì´í„° ë¶„ì„ê°€)

```sql
-- MySQL
SELECT
  event_name,
  COUNT(*) as count
FROM events
WHERE DATE(timestamp) = '2025-01-15'
GROUP BY event_name;

-- BigQuery
SELECT
  event_name,
  COUNT(*) as count
FROM `project.dataset.events_*`
WHERE _TABLE_SUFFIX = '20250115'
GROUP BY event_name;
```

##### ë°©ë²• 3: ë¶„ì„ í”Œë«í¼ ëŒ€ì‹œë³´ë“œ (ë¹„ê°œë°œì)

```
Amplitude ì ‘ì†
  â†“
"Events" ë©”ë‰´
  â†“
ë‚ ì§œ ì„ íƒ: 2025-01-15
  â†“
ì´ë²¤íŠ¸ë³„ ì§‘ê³„ í™•ì¸
  â†“
í•„ìš” ì‹œ CSV ë‹¤ìš´ë¡œë“œ
```

##### ë°©ë²• 4: Pythonìœ¼ë¡œ íŒŒì¼ ì½ê¸°

```python
import pandas as pd
import json

# CSV ì½ê¸°
df_csv = pd.read_csv('events.csv')
print(df_csv.head())

# JSONL ì½ê¸°
df_jsonl = pd.read_json('events.jsonl', lines=True)
print(df_jsonl.head())

# Parquet ì½ê¸°
df_parquet = pd.read_parquet('events.parquet')
print(df_parquet.head())

# ê¸°ë³¸ ë¶„ì„
print(df_parquet['event_name'].value_counts())
```

---

#### 6. ì‹¤ë¬´ íŒ

##### íŒ 1: ë°ì´í„° í˜•ì‹ ì„ íƒ ê°€ì´ë“œ

```
ì„ íƒ ê¸°ì¤€:

ë°ì´í„° í¬ê¸°ê°€ ì‘ë‹¤ (< 100MB)
  â†’ CSV (Excelë¡œ ì—´ê¸° ì‰¬ì›€)

ë³µì¡í•œ êµ¬ì¡° (ë°°ì—´, ì¤‘ì²© ê°ì²´)
  â†’ JSONL

ëŒ€ìš©ëŸ‰ (> 1GB)
  â†’ Parquet

ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
  â†’ JSONL (Kafka)

ì¥ê¸° ë³´ê´€
  â†’ Parquet (ì••ì¶•ë¥  ë†’ìŒ)

ê³µìœ /í˜‘ì—…
  â†’ CSV (ë²”ìš©ì„±)
```

##### íŒ 2: íŒŒí‹°ì…”ë‹

ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ë‚ ì§œë³„ë¡œ ë¶„í•  ì €ì¥:

```
# ì¢‹ì€ ì˜ˆ
s3://bucket/events/year=2025/month=01/day=15/data.parquet
s3://bucket/events/year=2025/month=01/day=16/data.parquet

# ë‚˜ìœ ì˜ˆ
s3://bucket/events/all_data.parquet  (ìˆ˜ TB í¬ê¸°)
```

**ì´ìœ :**
- íŠ¹ì • ë‚ ì§œë§Œ ì½ê¸° (ë¹„ìš© ì ˆê°)
- ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
- ê´€ë¦¬ ìš©ì´

##### íŒ 3: ì••ì¶• ì‚¬ìš©

```
# JSONL ì••ì¶•
gzip events.jsonl
â†’ events.jsonl.gz (70% í¬ê¸° ê°ì†Œ)

# Parquetì€ ìë™ ì••ì¶•
â†’ ë³„ë„ ì••ì¶• ë¶ˆí•„ìš”
```

##### íŒ 4: ìŠ¤í‚¤ë§ˆ ì •ì˜

ë°ì´í„° í˜•ì‹ì— ìŠ¤í‚¤ë§ˆ(êµ¬ì¡°) ì •ì˜ ì €ì¥:

```json
// events_schema.json
{
  "event_name": "string",
  "user_id": "string",
  "product_id": "string",
  "price": "integer",
  "timestamp": "timestamp"
}
```

**ì´ìœ :**
- ë°ì´í„° íƒ€ì… ê²€ì¦
- ë¬¸ì„œ ì—­í• 
- ìë™ í…Œì´ë¸” ìƒì„±

---

ì´ê²ƒìœ¼ë¡œ ë¶€ë¡ Bë¥¼ ë§ˆì¹©ë‹ˆë‹¤. ì´ì œ ì‹¤ë¬´ì—ì„œ ì–´ë–¤ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ë§Œë‚˜ë„ ë‹¹í™©í•˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.

---

## ë¶€ë¡ C-1: Python (Pandas) ì‹¤ìŠµ ê°€ì´ë“œ

> **ëª©í‘œ:** ì‹¤ì œ ë°ì´í„°ë¥¼ Pythonìœ¼ë¡œ ì½ê³ , ì •ì œí•˜ê³ , ë¶„ì„í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.

### 1. í™˜ê²½ ì¤€ë¹„

#### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
# ê¸°ë³¸ ë¶„ì„ ë„êµ¬
pip install pandas numpy

# ë°ì´í„° ì½ê¸°/ì“°ê¸°
pip install pyarrow fastparquet

# ì‹œê°í™”
pip install matplotlib seaborn plotly

# Jupyter Notebook (ì„ íƒ)
pip install jupyter
```

#### ì‘ì—… ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ events.csv
â”‚   â”œâ”€â”€ events.jsonl
â”‚   â””â”€â”€ events.parquet
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb
â””â”€â”€ output/
    â””â”€â”€ results.csv
```

---

### 2. ë°ì´í„° ì½ê¸° (3ê°€ì§€ í˜•ì‹)

#### 2-1. CSV ì½ê¸°

```python
import pandas as pd

# ê¸°ë³¸ ì½ê¸°
df_csv = pd.read_csv('data/events.csv')

# ì˜µì…˜ ì ìš©
df_csv = pd.read_csv(
    'data/events.csv',
    parse_dates=['timestamp'],  # ë‚ ì§œ ìë™ íŒŒì‹±
    dtype={
        'user_id': 'string',
        'product_id': 'string',
        'price': 'int64'
    }
)

# ë°ì´í„° í™•ì¸
print(df_csv.head())
print(df_csv.info())
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
   event_name  user_id product_id   price           timestamp
0 product_view user_001  PROD-123   89000 2025-01-15 10:00:00
1  add_to_cart user_001  PROD-123   89000 2025-01-15 10:02:30
2     purchase user_001  PROD-123   89000 2025-01-15 10:05:45
```

#### 2-2. JSONL ì½ê¸°

```python
# ê¸°ë³¸ ì½ê¸°
df_jsonl = pd.read_json('data/events.jsonl', lines=True)

# ì¤‘ì²©ëœ JSON êµ¬ì¡° í¼ì¹˜ê¸°
df_jsonl = pd.read_json('data/events.jsonl', lines=True)

# metadata ì»¬ëŸ¼ì´ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
if 'metadata' in df_jsonl.columns:
    # ë”•ì…”ë„ˆë¦¬ë¥¼ ë³„ë„ ì»¬ëŸ¼ìœ¼ë¡œ í™•ì¥
    metadata_df = pd.json_normalize(df_jsonl['metadata'])
    df_jsonl = pd.concat([df_jsonl.drop('metadata', axis=1), metadata_df], axis=1)

print(df_jsonl.head())
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
   event_name  user_id product_id   price platform    device
0 product_view user_001  PROD-123   89000      iOS iPhone 15
1  add_to_cart user_001  PROD-123   89000      iOS iPhone 15
```

#### 2-3. Parquet ì½ê¸° (ê°€ì¥ ë¹ ë¦„)

```python
# ê¸°ë³¸ ì½ê¸°
df_parquet = pd.read_parquet('data/events.parquet')

# íŠ¹ì • ì»¬ëŸ¼ë§Œ ì½ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½)
df_parquet = pd.read_parquet(
    'data/events.parquet',
    columns=['event_name', 'user_id', 'timestamp', 'price']
)

# í•„í„°ë§í•˜ë©° ì½ê¸°
df_parquet = pd.read_parquet(
    'data/events.parquet',
    filters=[('event_name', '==', 'purchase_complete')]
)

print(df_parquet.head())
```

---

### 3. ë°ì´í„° ì „ì²˜ë¦¬ (Preprocessing)

#### 3-1. ê¸°ë³¸ ì •ë³´ í™•ì¸

```python
# ë°ì´í„° í˜•íƒœ í™•ì¸
print(f"í–‰ ê°œìˆ˜: {len(df)}")
print(f"ì»¬ëŸ¼ ê°œìˆ˜: {len(df.columns)}")
print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# ê²°ì¸¡ì¹˜ í™•ì¸
print(df.isnull().sum())

# ì¤‘ë³µ í™•ì¸
print(f"ì¤‘ë³µ í–‰: {df.duplicated().sum()}")

# ë°ì´í„° íƒ€ì… í™•ì¸
print(df.dtypes)
```

#### 3-2. ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬

```python
# timestampë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
df['timestamp'] = pd.to_datetime(df['timestamp'])

# ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ ì¶”ê°€
df['date'] = df['timestamp'].dt.date
df['hour'] = df['timestamp'].dt.hour
df['day_of_week'] = df['timestamp'].dt.day_name()

# ì‹œê°„ ì°¨ì´ ê³„ì‚°
df = df.sort_values(['user_id', 'timestamp'])
df['time_diff_seconds'] = df.groupby('user_id')['timestamp'].diff().dt.total_seconds()

print(df[['user_id', 'event_name', 'timestamp', 'time_diff_seconds']].head(10))
```

#### 3-3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬

```python
# ê²°ì¸¡ì¹˜ í™•ì¸
missing = df.isnull().sum()
print(missing[missing > 0])

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „ëµ
# 1) ì‚­ì œ
df_clean = df.dropna(subset=['user_id', 'event_name'])

# 2) ê¸°ë³¸ê°’ ì±„ìš°ê¸°
df['product_id'] = df['product_id'].fillna('UNKNOWN')
df['price'] = df['price'].fillna(0)

# 3) ì•/ë’¤ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
df['session_id'] = df.groupby('user_id')['session_id'].ffill()
```

#### 3-4. ì¤‘ë³µ ì œê±°

```python
# ì™„ì „ ì¤‘ë³µ ì œê±°
df = df.drop_duplicates()

# íŠ¹ì • ì»¬ëŸ¼ ê¸°ì¤€ ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ë§Œ ìœ ì§€)
df = df.drop_duplicates(subset=['user_id', 'event_name', 'timestamp'], keep='first')

print(f"ì¤‘ë³µ ì œê±° í›„: {len(df)} í–‰")
```

---

### 4. ê¸°ë³¸ ë¶„ì„ (Basic Analysis)

#### 4-1. ì´ë²¤íŠ¸ ì§‘ê³„

```python
# ì´ë²¤íŠ¸ë³„ ë°œìƒ íšŸìˆ˜
event_counts = df['event_name'].value_counts()
print(event_counts)

# ë¹„ìœ¨ ê³„ì‚°
event_ratio = df['event_name'].value_counts(normalize=True) * 100
print(event_ratio.round(2))

# ë‚ ì§œë³„ ì´ë²¤íŠ¸ ìˆ˜
daily_events = df.groupby('date')['event_name'].count()
print(daily_events)
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
product_view         4250
add_to_cart          1820
checkout_start        950
purchase_complete     385
```

#### 4-2. ì‚¬ìš©ì ë¶„ì„

```python
# í™œì„± ì‚¬ìš©ì ìˆ˜
unique_users = df['user_id'].nunique()
print(f"ì „ì²´ ì‚¬ìš©ì: {unique_users}ëª…")

# ì‚¬ìš©ìë‹¹ í‰ê·  ì´ë²¤íŠ¸ ìˆ˜
events_per_user = df.groupby('user_id').size()
print(f"ì‚¬ìš©ìë‹¹ í‰ê·  ì´ë²¤íŠ¸: {events_per_user.mean():.2f}íšŒ")

# ê°€ì¥ í™œë°œí•œ ì‚¬ìš©ì TOP 10
top_users = events_per_user.sort_values(ascending=False).head(10)
print(top_users)

# ì‹ ê·œ vs ì¬ë°©ë¬¸ ì‚¬ìš©ì
purchase_users = df[df['event_name'] == 'purchase_complete']
first_purchase = purchase_users.groupby('user_id')['timestamp'].min()
purchase_users = purchase_users.merge(
    first_purchase.rename('first_purchase_time'),
    left_on='user_id',
    right_index=True
)
purchase_users['is_repeat'] = purchase_users['timestamp'] > purchase_users['first_purchase_time']
repeat_rate = purchase_users['is_repeat'].mean() * 100
print(f"ì¬êµ¬ë§¤ìœ¨: {repeat_rate:.2f}%")
```

#### 4-3. ë§¤ì¶œ ë¶„ì„

```python
# êµ¬ë§¤ ì´ë²¤íŠ¸ë§Œ í•„í„°ë§
purchases = df[df['event_name'] == 'purchase_complete'].copy()

# ì´ ë§¤ì¶œ
total_revenue = purchases['price'].sum()
print(f"ì´ ë§¤ì¶œ: {total_revenue:,}ì›")

# í‰ê·  ê°ë‹¨ê°€ (Average Order Value)
avg_order_value = purchases['price'].mean()
print(f"í‰ê·  ê°ë‹¨ê°€: {avg_order_value:,.0f}ì›")

# ì¼ë³„ ë§¤ì¶œ
daily_revenue = purchases.groupby('date')['price'].sum()
print(daily_revenue)

# ë§¤ì¶œ ë¶„í¬
print(purchases['price'].describe())

# ìƒìœ„ 10% ê³ ê°ì˜ ë§¤ì¶œ ê¸°ì—¬ë„
user_revenue = purchases.groupby('user_id')['price'].sum().sort_values(ascending=False)
top_10pct_count = int(len(user_revenue) * 0.1)
top_10pct_revenue = user_revenue.head(top_10pct_count).sum()
contribution = (top_10pct_revenue / total_revenue) * 100
print(f"ìƒìœ„ 10% ê³ ê° ë§¤ì¶œ ê¸°ì—¬ë„: {contribution:.1f}%")
```

---

### 5. í¼ë„ ë¶„ì„ (Funnel Analysis)

#### 5-1. ê¸°ë³¸ í¼ë„

```python
# í¼ë„ ë‹¨ê³„ ì •ì˜
funnel_steps = ['product_view', 'add_to_cart', 'checkout_start', 'purchase_complete']

# ê° ë‹¨ê³„ë³„ ì‚¬ìš©ì ìˆ˜
funnel_data = []
for step in funnel_steps:
    user_count = df[df['event_name'] == step]['user_id'].nunique()
    funnel_data.append({'step': step, 'users': user_count})

funnel_df = pd.DataFrame(funnel_data)

# ì „í™˜ìœ¨ ê³„ì‚°
funnel_df['conversion_rate'] = (funnel_df['users'] / funnel_df['users'].iloc[0] * 100).round(2)
funnel_df['step_conversion'] = (funnel_df['users'] / funnel_df['users'].shift(1) * 100).round(2)

print(funnel_df)
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
               step  users  conversion_rate  step_conversion
0     product_view    500            100.00              NaN
1     add_to_cart    215             43.00            43.00
2   checkout_start    108             21.60            50.23
3 purchase_complete     48              9.60            44.44
```

#### 5-2. ì‚¬ìš©ìë³„ í¼ë„ ë‹¬ì„±ë„

```python
# ê° ì‚¬ìš©ìê°€ ì–´ëŠ ë‹¨ê³„ê¹Œì§€ ë„ë‹¬í–ˆëŠ”ì§€
user_funnel = df[df['event_name'].isin(funnel_steps)].copy()
user_funnel['step_order'] = user_funnel['event_name'].map({
    'product_view': 1,
    'add_to_cart': 2,
    'checkout_start': 3,
    'purchase_complete': 4
})

# ì‚¬ìš©ìë³„ ìµœëŒ€ ë‹¨ê³„
user_max_step = user_funnel.groupby('user_id')['step_order'].max()

# ë‹¨ê³„ë³„ ì´íƒˆ ë¶„ì„
dropout_analysis = user_max_step.value_counts().sort_index()
dropout_df = pd.DataFrame({
    'max_step': dropout_analysis.index,
    'user_count': dropout_analysis.values,
    'step_name': [funnel_steps[i-1] for i in dropout_analysis.index]
})
dropout_df['dropout_rate'] = (dropout_df['user_count'] / dropout_df['user_count'].sum() * 100).round(2)

print(dropout_df)
```

#### 5-3. ì‹œê°„ëŒ€ë³„ í¼ë„ ë¹„êµ

```python
# ì‹œê°„ëŒ€ ì¶”ê°€
df['hour'] = df['timestamp'].dt.hour

# ì˜¤ì „(6-12ì‹œ) vs ì˜¤í›„(12-18ì‹œ) vs ì €ë…(18-24ì‹œ)
def time_period(hour):
    if 6 <= hour < 12:
        return 'ì˜¤ì „'
    elif 12 <= hour < 18:
        return 'ì˜¤í›„'
    elif 18 <= hour < 24:
        return 'ì €ë…'
    else:
        return 'ìƒˆë²½'

df['time_period'] = df['hour'].apply(time_period)

# ì‹œê°„ëŒ€ë³„ ì „í™˜ìœ¨
time_funnel = []
for period in ['ì˜¤ì „', 'ì˜¤í›„', 'ì €ë…', 'ìƒˆë²½']:
    period_df = df[df['time_period'] == period]
    views = period_df[period_df['event_name'] == 'product_view']['user_id'].nunique()
    purchases = period_df[period_df['event_name'] == 'purchase_complete']['user_id'].nunique()
    conversion = (purchases / views * 100) if views > 0 else 0
    time_funnel.append({
        'time_period': period,
        'views': views,
        'purchases': purchases,
        'conversion_rate': round(conversion, 2)
    })

time_funnel_df = pd.DataFrame(time_funnel)
print(time_funnel_df)
```

---

### 6. ì½”í˜¸íŠ¸ ë¶„ì„ (Cohort Analysis)

#### 6-1. ê°€ì… ì›”ë³„ ì½”í˜¸íŠ¸

```python
# ê° ì‚¬ìš©ìì˜ ì²« ì´ë²¤íŠ¸ ë‚ ì§œ (ê°€ì…ì¼ë¡œ ê°„ì£¼)
user_first_event = df.groupby('user_id')['timestamp'].min().reset_index()
user_first_event.columns = ['user_id', 'cohort_date']
user_first_event['cohort_month'] = user_first_event['cohort_date'].dt.to_period('M')

# ì›ë³¸ ë°ì´í„°ì— ì½”í˜¸íŠ¸ ì •ë³´ ì¶”ê°€
df = df.merge(user_first_event[['user_id', 'cohort_month']], on='user_id')
df['event_month'] = df['timestamp'].dt.to_period('M')

# ì½”í˜¸íŠ¸ë³„ ì›”ë³„ í™œì„± ì‚¬ìš©ì ìˆ˜
cohort_data = df.groupby(['cohort_month', 'event_month'])['user_id'].nunique().reset_index()
cohort_data.columns = ['cohort_month', 'event_month', 'active_users']

# í”¼ë²— í…Œì´ë¸”ë¡œ ë³€í™˜
cohort_pivot = cohort_data.pivot_table(
    index='cohort_month',
    columns='event_month',
    values='active_users'
)

print(cohort_pivot)
```

#### 6-2. ë¦¬í…ì…˜ (Retention) ê³„ì‚°

```python
# ì½”í˜¸íŠ¸ í¬ê¸° (ì²« ë‹¬ ì‚¬ìš©ì ìˆ˜)
cohort_sizes = cohort_pivot.iloc[:, 0]

# ë¦¬í…ì…˜ìœ¨ ê³„ì‚°
retention = cohort_pivot.divide(cohort_sizes, axis=0) * 100
retention = retention.round(2)

print("ë¦¬í…ì…˜ìœ¨ (%):")
print(retention)
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ë¦¬í…ì…˜ìœ¨ (%):
event_month    2025-01  2025-02  2025-03
cohort_month
2025-01         100.00    45.20    32.10
2025-02           NaN   100.00    48.50
2025-03           NaN      NaN   100.00
```

---

### 7. ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ (Segmentation)

#### 7-1. RFM ë¶„ì„ (Recency, Frequency, Monetary)

```python
# êµ¬ë§¤ ì´ë²¤íŠ¸ë§Œ ì¶”ì¶œ
purchases = df[df['event_name'] == 'purchase_complete'].copy()

# ë¶„ì„ ê¸°ì¤€ì¼ (ê°€ì¥ ìµœê·¼ ë‚ ì§œ)
analysis_date = purchases['timestamp'].max()

# ì‚¬ìš©ìë³„ RFM ê³„ì‚°
rfm = purchases.groupby('user_id').agg({
    'timestamp': lambda x: (analysis_date - x.max()).days,  # Recency
    'event_name': 'count',  # Frequency
    'price': 'sum'  # Monetary
}).reset_index()

rfm.columns = ['user_id', 'recency', 'frequency', 'monetary']

# RFM ì ìˆ˜ ê³„ì‚° (1-5ì )
rfm['r_score'] = pd.qcut(rfm['recency'], 5, labels=[5, 4, 3, 2, 1])
rfm['f_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 5, labels=[1, 2, 3, 4, 5])
rfm['m_score'] = pd.qcut(rfm['monetary'], 5, labels=[1, 2, 3, 4, 5])

# RFM ì´ì 
rfm['rfm_score'] = rfm['r_score'].astype(int) + rfm['f_score'].astype(int) + rfm['m_score'].astype(int)

# ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜
def rfm_segment(row):
    if row['rfm_score'] >= 12:
        return 'Champions'
    elif row['rfm_score'] >= 9:
        return 'Loyal Customers'
    elif row['rfm_score'] >= 6:
        return 'Potential'
    else:
        return 'At Risk'

rfm['segment'] = rfm.apply(rfm_segment, axis=1)

# ì„¸ê·¸ë¨¼íŠ¸ë³„ í†µê³„
segment_stats = rfm.groupby('segment').agg({
    'user_id': 'count',
    'recency': 'mean',
    'frequency': 'mean',
    'monetary': 'mean'
}).round(2)

print(segment_stats)
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
                  user_id  recency  frequency    monetary
segment
Champions              45     5.20       8.40  1250000.00
Loyal Customers        82    12.30       4.20   680000.00
Potential             120    25.40       2.10   320000.00
At Risk                53    48.20       1.30   150000.00
```

---

### 8. ì‹œê°í™” (Visualization)

#### 8-1. ì´ë²¤íŠ¸ ë¶„í¬ (ë§‰ëŒ€ ê·¸ë˜í”„)

```python
import matplotlib.pyplot as plt
import seaborn as sns

# í•œê¸€ í°íŠ¸ ì„¤ì • (Mac)
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# ì´ë²¤íŠ¸ë³„ ë°œìƒ íšŸìˆ˜
event_counts = df['event_name'].value_counts()

plt.figure(figsize=(10, 6))
event_counts.plot(kind='bar', color='skyblue')
plt.title('ì´ë²¤íŠ¸ë³„ ë°œìƒ íšŸìˆ˜', fontsize=16)
plt.xlabel('ì´ë²¤íŠ¸', fontsize=12)
plt.ylabel('íšŸìˆ˜', fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('output/event_distribution.png', dpi=300)
plt.show()
```

#### 8-2. ì¼ë³„ íŠ¸ë Œë“œ (ì„  ê·¸ë˜í”„)

```python
# ì¼ë³„ ì´ë²¤íŠ¸ ìˆ˜
daily_events = df.groupby(['date', 'event_name']).size().reset_index(name='count')

plt.figure(figsize=(12, 6))
for event in funnel_steps:
    event_data = daily_events[daily_events['event_name'] == event]
    plt.plot(event_data['date'], event_data['count'], marker='o', label=event)

plt.title('ì¼ë³„ ì´ë²¤íŠ¸ ë°œìƒ ì¶”ì´', fontsize=16)
plt.xlabel('ë‚ ì§œ', fontsize=12)
plt.ylabel('ì´ë²¤íŠ¸ ìˆ˜', fontsize=12)
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('output/daily_trend.png', dpi=300)
plt.show()
```

#### 8-3. í¼ë„ ì‹œê°í™”

```python
# í¼ë„ ë°ì´í„° (ì´ì „ì— ìƒì„±í•œ funnel_df ì‚¬ìš©)
plt.figure(figsize=(10, 8))

# í¼ë„ ì°¨íŠ¸
steps = funnel_df['step'].tolist()
users = funnel_df['users'].tolist()
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']

for i, (step, user_count, color) in enumerate(zip(steps, users, colors)):
    width = user_count / users[0]  # ì²« ë‹¨ê³„ ëŒ€ë¹„ ë¹„ìœ¨
    plt.barh(i, width, height=0.8, color=color, alpha=0.7)

    # ë¼ë²¨ ì¶”ê°€
    conversion = funnel_df.loc[i, 'conversion_rate']
    plt.text(width/2, i, f'{step}\n{user_count}ëª… ({conversion}%)',
             ha='center', va='center', fontsize=11, fontweight='bold')

plt.yticks([])
plt.xlim(0, 1)
plt.title('êµ¬ë§¤ í¼ë„ ì „í™˜ìœ¨', fontsize=16, fontweight='bold')
plt.xlabel('ì „í™˜ìœ¨ (%)', fontsize=12)
plt.tight_layout()
plt.savefig('output/funnel_chart.png', dpi=300)
plt.show()
```

#### 8-4. RFM íˆíŠ¸ë§µ

```python
# RFM ì„¸ê·¸ë¨¼íŠ¸ë³„ ì‚¬ìš©ì ìˆ˜
segment_counts = rfm['segment'].value_counts()

plt.figure(figsize=(8, 6))
plt.pie(segment_counts, labels=segment_counts.index, autopct='%1.1f%%',
        startangle=90, colors=sns.color_palette('pastel'))
plt.title('RFM ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬', fontsize=16)
plt.tight_layout()
plt.savefig('output/rfm_segments.png', dpi=300)
plt.show()
```

---

### 9. ë°ì´í„° ì €ì¥ (Export)

#### 9-1. CSVë¡œ ì €ì¥

```python
# ë¶„ì„ ê²°ê³¼ ì €ì¥
funnel_df.to_csv('output/funnel_analysis.csv', index=False, encoding='utf-8-sig')
rfm.to_csv('output/rfm_analysis.csv', index=False, encoding='utf-8-sig')

print("ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

#### 9-2. Excelë¡œ ì €ì¥ (ì—¬ëŸ¬ ì‹œíŠ¸)

```python
# Excel íŒŒì¼ë¡œ ì €ì¥
with pd.ExcelWriter('output/analysis_report.xlsx', engine='openpyxl') as writer:
    funnel_df.to_excel(writer, sheet_name='í¼ë„ë¶„ì„', index=False)
    rfm.to_excel(writer, sheet_name='RFMë¶„ì„', index=False)
    segment_stats.to_excel(writer, sheet_name='ì„¸ê·¸ë¨¼íŠ¸í†µê³„')
    daily_events.to_excel(writer, sheet_name='ì¼ë³„íŠ¸ë Œë“œ', index=False)

print("Excel ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: output/analysis_report.xlsx")
```

#### 9-3. Parquetë¡œ ì €ì¥ (ë¹ ë¥¸ ì¬ì‚¬ìš©)

```python
# ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
df.to_parquet('output/processed_events.parquet', compression='snappy', index=False)

# ë‚˜ì¤‘ì— ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ê¸°
df_reload = pd.read_parquet('output/processed_events.parquet')
print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_reload)} í–‰")
```

---

### 10. ì „ì²´ ì›Œí¬í”Œë¡œìš° (Complete Workflow)

ì•„ë˜ëŠ” ì „ì²´ ë¶„ì„ ê³¼ì •ì„ í•˜ë‚˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤:

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŠ¸ë˜í‚¹ ë°ì´í„° ë¶„ì„ ì „ì²´ ì›Œí¬í”Œë¡œìš°
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

def load_data(file_path, file_type='parquet'):
    """ë°ì´í„° ë¡œë“œ"""
    if file_type == 'csv':
        df = pd.read_csv(file_path, parse_dates=['timestamp'])
    elif file_type == 'jsonl':
        df = pd.read_json(file_path, lines=True)
    elif file_type == 'parquet':
        df = pd.read_parquet(file_path)

    print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)} í–‰, {len(df.columns)} ì»¬ëŸ¼")
    return df

def preprocess_data(df):
    """ë°ì´í„° ì „ì²˜ë¦¬"""
    # ë‚ ì§œ ë³€í™˜
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date
    df['hour'] = df['timestamp'].dt.hour

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df = df.dropna(subset=['user_id', 'event_name'])

    # ì¤‘ë³µ ì œê±°
    df = df.drop_duplicates()

    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)} í–‰")
    return df

def analyze_funnel(df, funnel_steps):
    """í¼ë„ ë¶„ì„"""
    funnel_data = []
    for step in funnel_steps:
        user_count = df[df['event_name'] == step]['user_id'].nunique()
        funnel_data.append({'step': step, 'users': user_count})

    funnel_df = pd.DataFrame(funnel_data)
    funnel_df['conversion_rate'] = (funnel_df['users'] / funnel_df['users'].iloc[0] * 100).round(2)

    return funnel_df

def analyze_rfm(df, analysis_date=None):
    """RFM ë¶„ì„"""
    purchases = df[df['event_name'] == 'purchase_complete'].copy()

    if analysis_date is None:
        analysis_date = purchases['timestamp'].max()

    rfm = purchases.groupby('user_id').agg({
        'timestamp': lambda x: (analysis_date - x.max()).days,
        'event_name': 'count',
        'price': 'sum'
    }).reset_index()

    rfm.columns = ['user_id', 'recency', 'frequency', 'monetary']

    # RFM ì ìˆ˜
    rfm['r_score'] = pd.qcut(rfm['recency'], 5, labels=[5, 4, 3, 2, 1])
    rfm['f_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 5, labels=[1, 2, 3, 4, 5])
    rfm['m_score'] = pd.qcut(rfm['monetary'], 5, labels=[1, 2, 3, 4, 5])

    return rfm

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # 1. ë°ì´í„° ë¡œë“œ
    df = load_data('data/events.parquet', file_type='parquet')

    # 2. ì „ì²˜ë¦¬
    df = preprocess_data(df)

    # 3. ê¸°ë³¸ í†µê³„
    print("\n=== ê¸°ë³¸ í†µê³„ ===")
    print(f"ì „ì²´ ì´ë²¤íŠ¸ ìˆ˜: {len(df):,}")
    print(f"ì „ì²´ ì‚¬ìš©ì ìˆ˜: {df['user_id'].nunique():,}")
    print(f"ë¶„ì„ ê¸°ê°„: {df['date'].min()} ~ {df['date'].max()}")

    # 4. í¼ë„ ë¶„ì„
    print("\n=== í¼ë„ ë¶„ì„ ===")
    funnel_steps = ['product_view', 'add_to_cart', 'checkout_start', 'purchase_complete']
    funnel_df = analyze_funnel(df, funnel_steps)
    print(funnel_df)

    # 5. RFM ë¶„ì„
    print("\n=== RFM ë¶„ì„ ===")
    rfm = analyze_rfm(df)
    print(rfm.head(10))

    # 6. ê²°ê³¼ ì €ì¥
    funnel_df.to_csv('output/funnel_analysis.csv', index=False, encoding='utf-8-sig')
    rfm.to_csv('output/rfm_analysis.csv', index=False, encoding='utf-8-sig')

    print("\në¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ output/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main()
```

**ì‹¤í–‰ ë°©ë²•:**

```bash
# Jupyter Notebookìœ¼ë¡œ ì‹¤í–‰
jupyter notebook notebooks/analysis.ipynb

# Python ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰
python analysis.py
```

---

### 11. ì‹¤ë¬´ íŒ

#### íŒ 1: ë©”ëª¨ë¦¬ ìµœì í™”

í° ë°ì´í„°ì…‹ì„ ë‹¤ë£° ë•ŒëŠ” ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•´ì•¼ í•©ë‹ˆë‹¤:

```python
# ë°ì´í„° íƒ€ì… ìµœì í™”
df['user_id'] = df['user_id'].astype('category')
df['event_name'] = df['event_name'].astype('category')
df['price'] = df['price'].astype('int32')  # int64 ëŒ€ì‹  int32

# ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸°
chunk_size = 100000
chunks = []
for chunk in pd.read_csv('data/large_events.csv', chunksize=chunk_size):
    # ê° ì²­í¬ ì²˜ë¦¬
    chunk_processed = chunk[chunk['event_name'] == 'purchase_complete']
    chunks.append(chunk_processed)

df = pd.concat(chunks, ignore_index=True)
```

#### íŒ 2: ì„±ëŠ¥ ì¸¡ì •

```python
import time

start = time.time()

# ë¶„ì„ ì½”ë“œ
df = pd.read_parquet('data/events.parquet')
result = df.groupby('event_name').size()

elapsed = time.time() - start
print(f"ì‹¤í–‰ ì‹œê°„: {elapsed:.2f}ì´ˆ")
```

#### íŒ 3: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ ë§Œë“¤ê¸°

```python
def calculate_conversion_rate(df, from_event, to_event):
    """ë‘ ì´ë²¤íŠ¸ ê°„ ì „í™˜ìœ¨ ê³„ì‚°"""
    from_users = df[df['event_name'] == from_event]['user_id'].nunique()
    to_users = df[df['event_name'] == to_event]['user_id'].nunique()

    if from_users == 0:
        return 0

    conversion = (to_users / from_users) * 100
    return round(conversion, 2)

# ì‚¬ìš© ì˜ˆì‹œ
view_to_cart = calculate_conversion_rate(df, 'product_view', 'add_to_cart')
cart_to_purchase = calculate_conversion_rate(df, 'add_to_cart', 'purchase_complete')

print(f"ì¡°íšŒâ†’ì¥ë°”êµ¬ë‹ˆ: {view_to_cart}%")
print(f"ì¥ë°”êµ¬ë‹ˆâ†’êµ¬ë§¤: {cart_to_purchase}%")
```

---

ì´ê²ƒìœ¼ë¡œ ë¶€ë¡ C-1ì„ ë§ˆì¹©ë‹ˆë‹¤. ì´ì œ Pythonê³¼ Pandasë¡œ ì‹¤ì œ ë°ì´í„°ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

---

## ë¶€ë¡ C-2: SQL (MySQL) ì‹¤ìŠµ ê°€ì´ë“œ

> **ëª©í‘œ:** SQLë¡œ íŠ¸ë˜í‚¹ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ì¡°íšŒí•˜ë©°, ì‹¤ë¬´ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë¶„ì„ ì¿¼ë¦¬ë¥¼ ìµí™ë‹ˆë‹¤.

### 1. í™˜ê²½ ì¤€ë¹„

#### MySQL ì„¤ì¹˜ (Mac)

```bash
# Homebrewë¡œ MySQL ì„¤ì¹˜
brew install mysql

# MySQL ì„œë¹„ìŠ¤ ì‹œì‘
brew services start mysql

# MySQL ì ‘ì† (ì´ˆê¸° ë¹„ë°€ë²ˆí˜¸ ì—†ìŒ)
mysql -u root

# root ë¹„ë°€ë²ˆí˜¸ ì„¤ì •
ALTER USER 'root'@'localhost' IDENTIFIED BY 'your_password';
```

#### MySQL ì ‘ì†

```bash
# ë¹„ë°€ë²ˆí˜¸ë¡œ ì ‘ì†
mysql -u root -p

# ë˜ëŠ” íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ ì ‘ì†
mysql -u root -p tracking_db
```

#### MySQL Workbench (GUI ë„êµ¬)

```bash
# GUI ë„êµ¬ ì„¤ì¹˜ (ì„ íƒ)
brew install --cask mysqlworkbench
```

---

### 2. ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” ìƒì„±

#### 2-1. ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±

```sql
-- ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
CREATE DATABASE tracking_db
  DEFAULT CHARACTER SET utf8mb4
  DEFAULT COLLATE utf8mb4_unicode_ci;

-- ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ
USE tracking_db;

-- ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ í™•ì¸
SHOW DATABASES;
```

#### 2-2. ì´ë²¤íŠ¸ í…Œì´ë¸” ìƒì„±

```sql
-- ì´ë²¤íŠ¸ í…Œì´ë¸”
CREATE TABLE events (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    event_name VARCHAR(100) NOT NULL,
    user_id VARCHAR(100) NOT NULL,
    session_id VARCHAR(100),
    timestamp DATETIME NOT NULL,
    platform VARCHAR(50),
    device_type VARCHAR(50),
    os_version VARCHAR(50),
    app_version VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_event_name (event_name),
    INDEX idx_user_id (user_id),
    INDEX idx_timestamp (timestamp)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- í…Œì´ë¸” êµ¬ì¡° í™•ì¸
DESCRIBE events;
```

**ì¸ë±ìŠ¤ ì„¤ëª…:**
- `idx_event_name`: ì´ë²¤íŠ¸ ì¢…ë¥˜ë³„ ì¡°íšŒ ìµœì í™”
- `idx_user_id`: ì‚¬ìš©ìë³„ ì¡°íšŒ ìµœì í™”
- `idx_timestamp`: ë‚ ì§œ ë²”ìœ„ ì¡°íšŒ ìµœì í™”

#### 2-3. êµ¬ë§¤ ì´ë²¤íŠ¸ í…Œì´ë¸” (ìƒì„¸ ì •ë³´)

```sql
-- êµ¬ë§¤ ì´ë²¤íŠ¸ ìƒì„¸ í…Œì´ë¸”
CREATE TABLE purchase_events (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    event_id BIGINT NOT NULL,
    user_id VARCHAR(100) NOT NULL,
    order_id VARCHAR(100) NOT NULL,
    product_id VARCHAR(100),
    product_name VARCHAR(255),
    category VARCHAR(100),
    price DECIMAL(10, 2) NOT NULL,
    quantity INT DEFAULT 1,
    total_amount DECIMAL(10, 2) NOT NULL,
    payment_method VARCHAR(50),
    timestamp DATETIME NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (event_id) REFERENCES events(id) ON DELETE CASCADE,
    INDEX idx_user_id (user_id),
    INDEX idx_order_id (order_id),
    INDEX idx_timestamp (timestamp)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
```

#### 2-4. ì‚¬ìš©ì í…Œì´ë¸”

```sql
-- ì‚¬ìš©ì ì •ë³´ í…Œì´ë¸”
CREATE TABLE users (
    user_id VARCHAR(100) PRIMARY KEY,
    first_seen DATETIME NOT NULL,
    last_seen DATETIME NOT NULL,
    total_events INT DEFAULT 0,
    total_purchases INT DEFAULT 0,
    total_revenue DECIMAL(10, 2) DEFAULT 0,
    platform VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_first_seen (first_seen),
    INDEX idx_last_seen (last_seen)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
```

---

### 3. ë°ì´í„° ì…ë ¥ (INSERT)

#### 3-1. ë‹¨ì¼ ì´ë²¤íŠ¸ ì…ë ¥

```sql
-- ì´ë²¤íŠ¸ ì‚½ì…
INSERT INTO events (event_name, user_id, session_id, timestamp, platform, device_type)
VALUES ('product_view', 'user_001', 'sess_abc123', '2025-01-15 10:00:00', 'iOS', 'mobile');

-- ì…ë ¥ í™•ì¸
SELECT * FROM events ORDER BY id DESC LIMIT 1;
```

#### 3-2. ì—¬ëŸ¬ ì´ë²¤íŠ¸ í•œ ë²ˆì— ì…ë ¥

```sql
-- ì—¬ëŸ¬ ì´ë²¤íŠ¸ ë™ì‹œ ì‚½ì…
INSERT INTO events (event_name, user_id, session_id, timestamp, platform, device_type)
VALUES
    ('product_view', 'user_001', 'sess_abc123', '2025-01-15 10:00:00', 'iOS', 'mobile'),
    ('add_to_cart', 'user_001', 'sess_abc123', '2025-01-15 10:02:30', 'iOS', 'mobile'),
    ('checkout_start', 'user_001', 'sess_abc123', '2025-01-15 10:05:00', 'iOS', 'mobile'),
    ('purchase_complete', 'user_001', 'sess_abc123', '2025-01-15 10:07:45', 'iOS', 'mobile');

-- ì…ë ¥ í™•ì¸
SELECT * FROM events WHERE user_id = 'user_001' ORDER BY timestamp;
```

#### 3-3. CSV íŒŒì¼ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

```sql
-- CSV íŒŒì¼ ë¡œë“œ (MySQLì˜ LOAD DATA ì‚¬ìš©)
LOAD DATA LOCAL INFILE '/path/to/events.csv'
INTO TABLE events
FIELDS TERMINATED BY ','
ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 ROWS
(event_name, user_id, session_id, timestamp, platform, device_type);

-- ë°ì´í„° ê°œìˆ˜ í™•ì¸
SELECT COUNT(*) FROM events;
```

#### 3-4. Pythonì—ì„œ ë°ì´í„° ì…ë ¥

```python
import mysql.connector
import pandas as pd

# MySQL ì—°ê²°
conn = mysql.connector.connect(
    host='localhost',
    user='root',
    password='your_password',
    database='tracking_db'
)

cursor = conn.cursor()

# DataFrameì—ì„œ ë°ì´í„° ì…ë ¥
df = pd.read_csv('events.csv')

insert_query = """
INSERT INTO events (event_name, user_id, session_id, timestamp, platform, device_type)
VALUES (%s, %s, %s, %s, %s, %s)
"""

for _, row in df.iterrows():
    cursor.execute(insert_query, (
        row['event_name'],
        row['user_id'],
        row['session_id'],
        row['timestamp'],
        row['platform'],
        row['device_type']
    ))

conn.commit()
cursor.close()
conn.close()

print(f"{len(df)} í–‰ ì…ë ¥ ì™„ë£Œ")
```

---

### 4. ê¸°ë³¸ ì¡°íšŒ (SELECT)

#### 4-1. ì „ì²´ ë°ì´í„° ì¡°íšŒ

```sql
-- ëª¨ë“  ì´ë²¤íŠ¸ ì¡°íšŒ (ìµœì‹  10ê°œ)
SELECT * FROM events
ORDER BY timestamp DESC
LIMIT 10;

-- íŠ¹ì • ì»¬ëŸ¼ë§Œ ì¡°íšŒ
SELECT event_name, user_id, timestamp
FROM events
LIMIT 10;
```

#### 4-2. ì¡°ê±´ë¶€ ì¡°íšŒ (WHERE)

```sql
-- íŠ¹ì • ì´ë²¤íŠ¸ë§Œ ì¡°íšŒ
SELECT * FROM events
WHERE event_name = 'purchase_complete'
ORDER BY timestamp DESC;

-- íŠ¹ì • ë‚ ì§œ ë²”ìœ„
SELECT * FROM events
WHERE timestamp BETWEEN '2025-01-01' AND '2025-01-31'
ORDER BY timestamp DESC;

-- íŠ¹ì • ì‚¬ìš©ìì˜ ì´ë²¤íŠ¸
SELECT event_name, timestamp
FROM events
WHERE user_id = 'user_001'
ORDER BY timestamp;

-- ì—¬ëŸ¬ ì¡°ê±´ (AND, OR)
SELECT * FROM events
WHERE event_name = 'purchase_complete'
  AND platform = 'iOS'
  AND timestamp >= '2025-01-01'
ORDER BY timestamp DESC;
```

#### 4-3. ì§‘ê³„ í•¨ìˆ˜ (COUNT, SUM, AVG)

```sql
-- ì „ì²´ ì´ë²¤íŠ¸ ìˆ˜
SELECT COUNT(*) AS total_events FROM events;

-- ì´ë²¤íŠ¸ë³„ ë°œìƒ íšŸìˆ˜
SELECT event_name, COUNT(*) AS count
FROM events
GROUP BY event_name
ORDER BY count DESC;

-- ì‚¬ìš©ì ìˆ˜
SELECT COUNT(DISTINCT user_id) AS total_users FROM events;

-- ì¼ë³„ ì´ë²¤íŠ¸ ìˆ˜
SELECT DATE(timestamp) AS date, COUNT(*) AS event_count
FROM events
GROUP BY DATE(timestamp)
ORDER BY date DESC;
```

---

### 5. í¼ë„ ë¶„ì„ (Funnel Analysis)

#### 5-1. ê¸°ë³¸ í¼ë„

```sql
-- ê° ë‹¨ê³„ë³„ ì‚¬ìš©ì ìˆ˜
SELECT
    SUM(CASE WHEN event_name = 'product_view' THEN 1 ELSE 0 END) AS product_view,
    SUM(CASE WHEN event_name = 'add_to_cart' THEN 1 ELSE 0 END) AS add_to_cart,
    SUM(CASE WHEN event_name = 'checkout_start' THEN 1 ELSE 0 END) AS checkout_start,
    SUM(CASE WHEN event_name = 'purchase_complete' THEN 1 ELSE 0 END) AS purchase_complete
FROM events;

-- ê° ë‹¨ê³„ë³„ ê³ ìœ  ì‚¬ìš©ì ìˆ˜
SELECT
    COUNT(DISTINCT CASE WHEN event_name = 'product_view' THEN user_id END) AS product_view_users,
    COUNT(DISTINCT CASE WHEN event_name = 'add_to_cart' THEN user_id END) AS add_to_cart_users,
    COUNT(DISTINCT CASE WHEN event_name = 'checkout_start' THEN user_id END) AS checkout_start_users,
    COUNT(DISTINCT CASE WHEN event_name = 'purchase_complete' THEN user_id END) AS purchase_users
FROM events;
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
product_view_users | add_to_cart_users | checkout_start_users | purchase_users
        500        |        215        |         108         |       48
```

#### 5-2. ì „í™˜ìœ¨ ê³„ì‚°

```sql
-- ì „í™˜ìœ¨ ê³„ì‚° (ì„œë¸Œì¿¼ë¦¬ ì‚¬ìš©)
SELECT
    product_view_users,
    add_to_cart_users,
    checkout_start_users,
    purchase_users,
    ROUND(add_to_cart_users / product_view_users * 100, 2) AS view_to_cart_rate,
    ROUND(checkout_start_users / add_to_cart_users * 100, 2) AS cart_to_checkout_rate,
    ROUND(purchase_users / checkout_start_users * 100, 2) AS checkout_to_purchase_rate,
    ROUND(purchase_users / product_view_users * 100, 2) AS overall_conversion_rate
FROM (
    SELECT
        COUNT(DISTINCT CASE WHEN event_name = 'product_view' THEN user_id END) AS product_view_users,
        COUNT(DISTINCT CASE WHEN event_name = 'add_to_cart' THEN user_id END) AS add_to_cart_users,
        COUNT(DISTINCT CASE WHEN event_name = 'checkout_start' THEN user_id END) AS checkout_start_users,
        COUNT(DISTINCT CASE WHEN event_name = 'purchase_complete' THEN user_id END) AS purchase_users
    FROM events
) AS funnel_data;
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
view_to_cart_rate | cart_to_checkout_rate | checkout_to_purchase_rate | overall_conversion_rate
      43.00       |        50.23          |           44.44           |         9.60
```

#### 5-3. ì¼ë³„ í¼ë„ ì „í™˜ìœ¨

```sql
-- ì¼ë³„ í¼ë„ ë¶„ì„
SELECT
    DATE(timestamp) AS date,
    COUNT(DISTINCT CASE WHEN event_name = 'product_view' THEN user_id END) AS views,
    COUNT(DISTINCT CASE WHEN event_name = 'purchase_complete' THEN user_id END) AS purchases,
    ROUND(
        COUNT(DISTINCT CASE WHEN event_name = 'purchase_complete' THEN user_id END) /
        COUNT(DISTINCT CASE WHEN event_name = 'product_view' THEN user_id END) * 100,
        2
    ) AS conversion_rate
FROM events
GROUP BY DATE(timestamp)
ORDER BY date DESC;
```

---

### 6. ì‚¬ìš©ì í–‰ë™ ë¶„ì„

#### 6-1. ì‚¬ìš©ìë³„ ì´ë²¤íŠ¸ ìˆ˜

```sql
-- ì‚¬ìš©ìë³„ ì´ë²¤íŠ¸ ë°œìƒ íšŸìˆ˜
SELECT
    user_id,
    COUNT(*) AS total_events,
    COUNT(DISTINCT event_name) AS unique_events,
    MIN(timestamp) AS first_event,
    MAX(timestamp) AS last_event
FROM events
GROUP BY user_id
ORDER BY total_events DESC
LIMIT 20;
```

#### 6-2. ì‚¬ìš©ì ì—¬ì • ì¶”ì 

```sql
-- íŠ¹ì • ì‚¬ìš©ìì˜ ì „ì²´ ì—¬ì •
SELECT
    user_id,
    event_name,
    timestamp,
    LAG(event_name) OVER (PARTITION BY user_id ORDER BY timestamp) AS previous_event,
    TIMESTAMPDIFF(SECOND,
        LAG(timestamp) OVER (PARTITION BY user_id ORDER BY timestamp),
        timestamp
    ) AS seconds_from_previous
FROM events
WHERE user_id = 'user_001'
ORDER BY timestamp;
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
user_id  | event_name        | timestamp           | previous_event   | seconds_from_previous
user_001 | product_view      | 2025-01-15 10:00:00 | NULL            | NULL
user_001 | add_to_cart       | 2025-01-15 10:02:30 | product_view    | 150
user_001 | checkout_start    | 2025-01-15 10:05:00 | add_to_cart     | 150
user_001 | purchase_complete | 2025-01-15 10:07:45 | checkout_start  | 165
```

#### 6-3. í‰ê·  êµ¬ë§¤ ì‹œê°„

```sql
-- ì²« ì¡°íšŒë¶€í„° êµ¬ë§¤ê¹Œì§€ í‰ê·  ì‹œê°„
SELECT
    AVG(time_to_purchase) / 60 AS avg_minutes_to_purchase
FROM (
    SELECT
        user_id,
        TIMESTAMPDIFF(SECOND,
            MIN(CASE WHEN event_name = 'product_view' THEN timestamp END),
            MIN(CASE WHEN event_name = 'purchase_complete' THEN timestamp END)
        ) AS time_to_purchase
    FROM events
    GROUP BY user_id
    HAVING time_to_purchase IS NOT NULL
) AS purchase_times;
```

---

### 7. ë§¤ì¶œ ë¶„ì„

#### 7-1. ì¼ë³„ ë§¤ì¶œ

```sql
-- ì¼ë³„ ì´ ë§¤ì¶œ (purchase_events í…Œì´ë¸” ì‚¬ìš©)
SELECT
    DATE(timestamp) AS date,
    COUNT(DISTINCT order_id) AS total_orders,
    SUM(total_amount) AS daily_revenue,
    AVG(total_amount) AS avg_order_value,
    COUNT(DISTINCT user_id) AS purchasing_users
FROM purchase_events
GROUP BY DATE(timestamp)
ORDER BY date DESC;
```

#### 7-2. ìƒí’ˆë³„ ë§¤ì¶œ

```sql
-- ìƒí’ˆë³„ ë§¤ì¶œ ìˆœìœ„
SELECT
    product_id,
    product_name,
    COUNT(*) AS purchase_count,
    SUM(quantity) AS total_quantity,
    SUM(total_amount) AS total_revenue,
    AVG(price) AS avg_price
FROM purchase_events
GROUP BY product_id, product_name
ORDER BY total_revenue DESC
LIMIT 10;
```

#### 7-3. ì‚¬ìš©ìë³„ ì´ êµ¬ë§¤ì•¡ (LTV)

```sql
-- ì‚¬ìš©ìë³„ ìƒì•  ê°€ì¹˜ (Lifetime Value)
SELECT
    user_id,
    COUNT(DISTINCT order_id) AS purchase_count,
    SUM(total_amount) AS lifetime_value,
    AVG(total_amount) AS avg_order_value,
    MIN(timestamp) AS first_purchase,
    MAX(timestamp) AS last_purchase,
    DATEDIFF(MAX(timestamp), MIN(timestamp)) AS customer_lifespan_days
FROM purchase_events
GROUP BY user_id
ORDER BY lifetime_value DESC
LIMIT 20;
```

---

### 8. ê³ ê¸‰ ì¿¼ë¦¬ (Advanced Queries)

#### 8-1. ì½”í˜¸íŠ¸ ë¶„ì„ (Cohort Analysis)

```sql
-- ê°€ì… ì›”ë³„ ì½”í˜¸íŠ¸ ë¦¬í…ì…˜
WITH user_cohorts AS (
    SELECT
        user_id,
        DATE_FORMAT(MIN(timestamp), '%Y-%m') AS cohort_month
    FROM events
    GROUP BY user_id
),
cohort_activity AS (
    SELECT
        uc.cohort_month,
        DATE_FORMAT(e.timestamp, '%Y-%m') AS activity_month,
        COUNT(DISTINCT e.user_id) AS active_users
    FROM user_cohorts uc
    JOIN events e ON uc.user_id = e.user_id
    GROUP BY uc.cohort_month, DATE_FORMAT(e.timestamp, '%Y-%m')
)
SELECT
    cohort_month,
    activity_month,
    active_users
FROM cohort_activity
ORDER BY cohort_month, activity_month;
```

#### 8-2. RFM ë¶„ì„

```sql
-- RFM (Recency, Frequency, Monetary) ê³„ì‚°
SELECT
    user_id,
    DATEDIFF(NOW(), MAX(timestamp)) AS recency_days,
    COUNT(DISTINCT order_id) AS frequency,
    SUM(total_amount) AS monetary
FROM purchase_events
GROUP BY user_id
ORDER BY monetary DESC
LIMIT 20;
```

#### 8-3. ì„¸ì…˜ ë¶„ì„

```sql
-- ì„¸ì…˜ë‹¹ í‰ê·  ì´ë²¤íŠ¸ ìˆ˜
SELECT
    session_id,
    COUNT(*) AS events_in_session,
    COUNT(DISTINCT event_name) AS unique_events,
    TIMESTAMPDIFF(MINUTE, MIN(timestamp), MAX(timestamp)) AS session_duration_minutes
FROM events
WHERE session_id IS NOT NULL
GROUP BY session_id
HAVING session_duration_minutes > 0
ORDER BY events_in_session DESC
LIMIT 20;
```

---

### 9. ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ì‚­ì œ

#### 9-1. ë°ì´í„° ì—…ë°ì´íŠ¸ (UPDATE)

```sql
-- íŠ¹ì • ì‚¬ìš©ìì˜ í”Œë«í¼ ì •ë³´ ìˆ˜ì •
UPDATE events
SET platform = 'Android'
WHERE user_id = 'user_001' AND platform IS NULL;

-- ì—¬ëŸ¬ ì»¬ëŸ¼ ë™ì‹œ ì—…ë°ì´íŠ¸
UPDATE events
SET
    platform = 'iOS',
    device_type = 'mobile'
WHERE user_id = 'user_002';

-- ì¡°ê±´ë¶€ ì—…ë°ì´íŠ¸
UPDATE users
SET total_events = (
    SELECT COUNT(*) FROM events WHERE events.user_id = users.user_id
);
```

#### 9-2. ë°ì´í„° ì‚­ì œ (DELETE)

```sql
-- íŠ¹ì • ì¡°ê±´ì˜ ë°ì´í„° ì‚­ì œ
DELETE FROM events
WHERE event_name = 'test_event';

-- ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ (90ì¼ ì´ìƒ)
DELETE FROM events
WHERE timestamp < DATE_SUB(NOW(), INTERVAL 90 DAY);

-- ì•ˆì „í•œ ì‚­ì œ (íŠ¸ëœì­ì…˜ ì‚¬ìš©)
START TRANSACTION;
DELETE FROM events WHERE id = 12345;
-- í™•ì¸ í›„ ì»¤ë°‹ ë˜ëŠ” ë¡¤ë°±
COMMIT;
-- ë˜ëŠ”
ROLLBACK;
```

---

### 10. ì„±ëŠ¥ ìµœì í™”

#### 10-1. ì¸ë±ìŠ¤ ì¶”ê°€

```sql
-- ë³µí•© ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_user_event_time
ON events (user_id, event_name, timestamp);

-- ì¸ë±ìŠ¤ í™•ì¸
SHOW INDEX FROM events;

-- ì¸ë±ìŠ¤ ì‚­ì œ
DROP INDEX idx_user_event_time ON events;
```

#### 10-2. ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš í™•ì¸

```sql
-- EXPLAINìœ¼ë¡œ ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„
EXPLAIN SELECT * FROM events
WHERE user_id = 'user_001'
  AND timestamp >= '2025-01-01'
ORDER BY timestamp DESC;
```

#### 10-3. íŒŒí‹°ì…”ë‹ (ëŒ€ìš©ëŸ‰ ë°ì´í„°)

```sql
-- ë‚ ì§œ ê¸°ì¤€ íŒŒí‹°ì…˜ í…Œì´ë¸” ìƒì„±
CREATE TABLE events_partitioned (
    id BIGINT AUTO_INCREMENT,
    event_name VARCHAR(100) NOT NULL,
    user_id VARCHAR(100) NOT NULL,
    timestamp DATETIME NOT NULL,
    platform VARCHAR(50),
    PRIMARY KEY (id, timestamp),
    INDEX idx_user_id (user_id),
    INDEX idx_event_name (event_name)
) ENGINE=InnoDB
PARTITION BY RANGE (YEAR(timestamp) * 100 + MONTH(timestamp)) (
    PARTITION p202501 VALUES LESS THAN (202502),
    PARTITION p202502 VALUES LESS THAN (202503),
    PARTITION p202503 VALUES LESS THAN (202504),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

---

### 11. ì‹¤ë¬´ í™œìš© ì˜ˆì‹œ

#### ì˜ˆì‹œ 1: ì¼ì¼ ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬

```sql
-- ì˜¤ëŠ˜ì˜ ì£¼ìš” ì§€í‘œ
SELECT
    COUNT(*) AS total_events,
    COUNT(DISTINCT user_id) AS active_users,
    COUNT(DISTINCT CASE WHEN event_name = 'purchase_complete' THEN user_id END) AS purchasers,
    ROUND(
        COUNT(DISTINCT CASE WHEN event_name = 'purchase_complete' THEN user_id END) /
        COUNT(DISTINCT user_id) * 100,
        2
    ) AS conversion_rate
FROM events
WHERE DATE(timestamp) = CURDATE();
```

#### ì˜ˆì‹œ 2: ì£¼ê°„ íŠ¸ë Œë“œ ë¦¬í¬íŠ¸

```sql
-- ìµœê·¼ 7ì¼ê°„ ì¼ë³„ ì¶”ì´
SELECT
    DATE(timestamp) AS date,
    COUNT(*) AS events,
    COUNT(DISTINCT user_id) AS users,
    COUNT(DISTINCT CASE WHEN event_name = 'purchase_complete' THEN user_id END) AS purchasers
FROM events
WHERE timestamp >= DATE_SUB(CURDATE(), INTERVAL 7 DAY)
GROUP BY DATE(timestamp)
ORDER BY date;
```

#### ì˜ˆì‹œ 3: ì´íƒˆ ì‚¬ìš©ì ì°¾ê¸°

```sql
-- 30ì¼ ì´ìƒ í™œë™ì´ ì—†ëŠ” ì‚¬ìš©ì
SELECT
    user_id,
    MAX(timestamp) AS last_seen,
    DATEDIFF(NOW(), MAX(timestamp)) AS days_inactive,
    COUNT(*) AS total_events
FROM events
GROUP BY user_id
HAVING days_inactive >= 30
ORDER BY days_inactive DESC;
```

---

### 12. ë°±ì—… ë° ë³µì›

#### ë°ì´í„° ë°±ì—…

```bash
# ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
mysqldump -u root -p tracking_db > backup_tracking_db.sql

# íŠ¹ì • í…Œì´ë¸”ë§Œ ë°±ì—…
mysqldump -u root -p tracking_db events > backup_events.sql

# ë°ì´í„°ë§Œ ë°±ì—… (êµ¬ì¡° ì œì™¸)
mysqldump -u root -p --no-create-info tracking_db events > backup_events_data.sql
```

#### ë°ì´í„° ë³µì›

```bash
# ë°±ì—… íŒŒì¼ë¡œë¶€í„° ë³µì›
mysql -u root -p tracking_db < backup_tracking_db.sql

# ìƒˆ ë°ì´í„°ë² ì´ìŠ¤ì— ë³µì›
mysql -u root -p -e "CREATE DATABASE tracking_db_restored"
mysql -u root -p tracking_db_restored < backup_tracking_db.sql
```

---

### 13. ì‹¤ë¬´ íŒ

#### íŒ 1: íŠ¸ëœì­ì…˜ ì‚¬ìš©

ì¤‘ìš”í•œ ì‘ì—…ì€ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ì„¸ìš”:

```sql
START TRANSACTION;

-- ì—¬ëŸ¬ ì‘ì—… ìˆ˜í–‰
INSERT INTO events (...) VALUES (...);
UPDATE users SET total_events = total_events + 1 WHERE user_id = 'user_001';

-- ë¬¸ì œ ì—†ìœ¼ë©´ ì»¤ë°‹
COMMIT;

-- ë¬¸ì œ ë°œìƒ ì‹œ ë¡¤ë°±
ROLLBACK;
```

#### íŒ 2: ëŠë¦° ì¿¼ë¦¬ ë¡œê¹…

```sql
-- ëŠë¦° ì¿¼ë¦¬ ë¡œê·¸ í™œì„±í™”
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL long_query_time = 2;  -- 2ì´ˆ ì´ìƒ ê±¸ë¦¬ëŠ” ì¿¼ë¦¬ ê¸°ë¡

-- ëŠë¦° ì¿¼ë¦¬ ë¡œê·¸ ìœ„ì¹˜ í™•ì¸
SHOW VARIABLES LIKE 'slow_query_log_file';
```

#### íŒ 3: ì •ê¸°ì ì¸ í…Œì´ë¸” ìµœì í™”

```sql
-- í…Œì´ë¸” ìµœì í™”
OPTIMIZE TABLE events;

-- í…Œì´ë¸” ë¶„ì„ (í†µê³„ ì—…ë°ì´íŠ¸)
ANALYZE TABLE events;
```

---

ì´ê²ƒìœ¼ë¡œ ë¶€ë¡ C-2ë¥¼ ë§ˆì¹©ë‹ˆë‹¤. ì´ì œ SQLë¡œ íŠ¸ë˜í‚¹ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

---

## ë¶€ë¡ C-3: BigQuery ì‹¤ìŠµ ê°€ì´ë“œ

> **ëª©í‘œ:** Google BigQueryë¡œ ëŒ€ìš©ëŸ‰ íŠ¸ë˜í‚¹ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ë¶„ì„í•˜ê³ , GA4/Amplitude ë°ì´í„°ë¥¼ ì‹¤ë¬´ì—ì„œ í™œìš©í•©ë‹ˆë‹¤.

### 1. BigQuery ì‹œì‘í•˜ê¸°

#### 1-1. BigQueryë€?

BigQueryëŠ” Google Cloud Platformì˜ ì„œë²„ë¦¬ìŠ¤ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì…ë‹ˆë‹¤:

**íŠ¹ì§•:**
- **ë¹ ë¥¸ ì†ë„**: í…Œë¼ë°”ì´íŠ¸ê¸‰ ë°ì´í„°ë„ ì´ˆ ë‹¨ìœ„ ì¡°íšŒ
- **ìë™ í™•ì¥**: ì¸í”„ë¼ ê´€ë¦¬ ë¶ˆí•„ìš”
- **í‘œì¤€ SQL**: ìµìˆ™í•œ SQL ë¬¸ë²• ì‚¬ìš©
- **ë¹„ìš© íš¨ìœ¨**: ì¿¼ë¦¬í•œ ë°ì´í„°ì–‘ë§Œí¼ë§Œ ê³¼ê¸ˆ

**ì‹¤ë¬´ í™œìš©:**
- Google Analytics 4 ë°ì´í„° ë¶„ì„
- Amplitude Export ë°ì´í„° ë¶„ì„
- ëŒ€ê·œëª¨ íŠ¸ë˜í‚¹ ë¡œê·¸ ë¶„ì„

#### 1-2. GCP í”„ë¡œì íŠ¸ ìƒì„±

```bash
# 1. https://console.cloud.google.com ì ‘ì†
# 2. ìƒˆ í”„ë¡œì íŠ¸ ìƒì„± (ì˜ˆ: tracking-analytics)
# 3. ê²°ì œ ê³„ì • ì—°ê²° (ë¬´ë£Œ í¬ë ˆë”§ $300 ì œê³µ)
# 4. BigQuery API í™œì„±í™”
```

#### 1-3. ì ‘ì† ë°©ë²•

**ì›¹ ì½˜ì†”:**
- https://console.cloud.google.com/bigquery

**gcloud CLI ì„¤ì¹˜:**
```bash
# macOS
brew install --cask google-cloud-sdk

# ì¸ì¦
gcloud auth login
gcloud config set project tracking-analytics
```

**Python ë¼ì´ë¸ŒëŸ¬ë¦¬:**
```bash
pip install google-cloud-bigquery pandas-gbq
```

---

### 2. ë°ì´í„°ì…‹ ë° í…Œì´ë¸” ìƒì„±

#### 2-1. ë°ì´í„°ì…‹ ìƒì„±

```sql
-- ì›¹ ì½˜ì†”ì—ì„œ ìƒì„±í•˜ê±°ë‚˜ SQLë¡œ ìƒì„±
CREATE SCHEMA IF NOT EXISTS `tracking-analytics.tracking_data`
OPTIONS(
  location="us",
  description="íŠ¸ë˜í‚¹ ë°ì´í„° ì €ì¥ì†Œ"
);
```

#### 2-2. ì´ë²¤íŠ¸ í…Œì´ë¸” ìƒì„±

```sql
-- ê¸°ë³¸ ì´ë²¤íŠ¸ í…Œì´ë¸”
CREATE OR REPLACE TABLE `tracking-analytics.tracking_data.events` (
  event_id STRING,
  event_name STRING NOT NULL,
  user_id STRING NOT NULL,
  session_id STRING,
  timestamp TIMESTAMP NOT NULL,

  -- ì»¨í…ìŠ¤íŠ¸ ì •ë³´
  platform STRING,
  device_type STRING,
  os_version STRING,
  app_version STRING,

  -- ì‚¬ìš©ì ì†ì„± (STRUCT)
  user_properties STRUCT<
    membership_tier STRING,
    signup_date DATE,
    country STRING
  >,

  -- ì´ë²¤íŠ¸ íŒŒë¼ë¯¸í„° (ARRAY)
  event_params ARRAY<STRUCT<
    key STRING,
    value STRUCT<
      string_value STRING,
      int_value INT64,
      float_value FLOAT64,
      double_value FLOAT64
    >
  >>
)
PARTITION BY DATE(timestamp)
CLUSTER BY event_name, user_id
OPTIONS(
  description="ì‚¬ìš©ì ì´ë²¤íŠ¸ ë¡œê·¸",
  require_partition_filter=true
);
```

**ì¤‘ìš” ê°œë…:**
- `PARTITION BY DATE(timestamp)`: ë‚ ì§œë³„ë¡œ ë°ì´í„° ë¶„í•  â†’ ì¿¼ë¦¬ ë¹„ìš© ì ˆê°
- `CLUSTER BY`: íŠ¹ì • ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬ â†’ ì¿¼ë¦¬ ì„±ëŠ¥ í–¥ìƒ
- `require_partition_filter=true`: íŒŒí‹°ì…˜ í•„í„° í•„ìˆ˜ â†’ ì „ì²´ ìŠ¤ìº” ë°©ì§€

#### 2-3. êµ¬ë§¤ ì´ë²¤íŠ¸ í…Œì´ë¸”

```sql
-- êµ¬ë§¤ ì „ìš© í…Œì´ë¸” (í”Œë« êµ¬ì¡°)
CREATE OR REPLACE TABLE `tracking-analytics.tracking_data.purchases` (
  purchase_id STRING,
  user_id STRING NOT NULL,
  order_id STRING NOT NULL,
  timestamp TIMESTAMP NOT NULL,

  -- ìƒí’ˆ ì •ë³´
  product_id STRING,
  product_name STRING,
  category STRING,

  -- ê¸ˆì•¡ ì •ë³´
  price NUMERIC(10, 2),
  quantity INT64,
  total_amount NUMERIC(10, 2),
  currency STRING DEFAULT 'KRW',

  -- ê²°ì œ ì •ë³´
  payment_method STRING,
  payment_provider STRING
)
PARTITION BY DATE(timestamp)
CLUSTER BY user_id, product_id;
```

---

### 3. ë°ì´í„° ì…ë ¥

#### 3-1. SQLë¡œ ì§ì ‘ ì…ë ¥

```sql
-- ë‹¨ì¼ í–‰ ì‚½ì…
INSERT INTO `tracking-analytics.tracking_data.events`
  (event_id, event_name, user_id, session_id, timestamp, platform, device_type, event_params)
VALUES
  (
    GENERATE_UUID(),
    'product_view',
    'user_001',
    'sess_abc123',
    TIMESTAMP('2025-01-15 10:00:00'),
    'iOS',
    'mobile',
    [
      STRUCT('product_id' AS key, STRUCT('PROD-123' AS string_value, NULL AS int_value, NULL AS float_value, NULL AS double_value) AS value),
      STRUCT('price' AS key, STRUCT(NULL AS string_value, 89000 AS int_value, NULL AS float_value, NULL AS double_value) AS value)
    ]
  );
```

#### 3-2. CSV íŒŒì¼ ì—…ë¡œë“œ

```bash
# CSV íŒŒì¼ì„ BigQueryë¡œ ë¡œë“œ
bq load \
  --source_format=CSV \
  --skip_leading_rows=1 \
  --autodetect \
  tracking-analytics:tracking_data.events \
  gs://my-bucket/events.csv
```

#### 3-3. Pythonìœ¼ë¡œ ë°ì´í„° ì…ë ¥

```python
from google.cloud import bigquery
import pandas as pd

# BigQuery í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = bigquery.Client(project='tracking-analytics')

# DataFrame ìƒì„±
df = pd.DataFrame({
    'event_id': ['evt_001', 'evt_002', 'evt_003'],
    'event_name': ['product_view', 'add_to_cart', 'purchase_complete'],
    'user_id': ['user_001', 'user_001', 'user_001'],
    'timestamp': pd.to_datetime([
        '2025-01-15 10:00:00',
        '2025-01-15 10:02:30',
        '2025-01-15 10:07:45'
    ]),
    'platform': ['iOS', 'iOS', 'iOS']
})

# BigQueryì— ì‚½ì…
table_id = 'tracking-analytics.tracking_data.events'
job = client.load_table_from_dataframe(df, table_id)
job.result()  # ì™„ë£Œ ëŒ€ê¸°

print(f"{len(df)} í–‰ ì…ë ¥ ì™„ë£Œ")
```

#### 3-4. JSON Lines íŒŒì¼ ë¡œë“œ (ì‹¤ë¬´ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©)

```sql
-- GCSì˜ JSONL íŒŒì¼ì„ í…Œì´ë¸”ë¡œ ë¡œë“œ
LOAD DATA OVERWRITE `tracking-analytics.tracking_data.events`
FROM FILES (
  format = 'JSON',
  uris = ['gs://my-bucket/events/*.jsonl']
);
```

---

### 4. ê¸°ë³¸ ì¿¼ë¦¬

#### 4-1. ì „ì²´ ë°ì´í„° ì¡°íšŒ

```sql
-- ìµœê·¼ 10ê°œ ì´ë²¤íŠ¸ ì¡°íšŒ
SELECT
  event_name,
  user_id,
  timestamp,
  platform
FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) = '2025-01-15'  -- íŒŒí‹°ì…˜ í•„í„° í•„ìˆ˜
ORDER BY timestamp DESC
LIMIT 10;
```

#### 4-2. ì´ë²¤íŠ¸ ì§‘ê³„

```sql
-- ì´ë²¤íŠ¸ë³„ ë°œìƒ íšŸìˆ˜
SELECT
  event_name,
  COUNT(*) AS event_count,
  COUNT(DISTINCT user_id) AS unique_users
FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) BETWEEN '2025-01-01' AND '2025-01-31'
GROUP BY event_name
ORDER BY event_count DESC;
```

#### 4-3. ì¼ë³„ ì¶”ì´

```sql
-- ì¼ë³„ ì´ë²¤íŠ¸ ë°œìƒ ì¶”ì´
SELECT
  DATE(timestamp) AS date,
  event_name,
  COUNT(*) AS event_count,
  COUNT(DISTINCT user_id) AS unique_users
FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
GROUP BY date, event_name
ORDER BY date DESC, event_count DESC;
```

---

### 5. UNNESTë¡œ ë°°ì—´/êµ¬ì¡°ì²´ ë‹¤ë£¨ê¸°

BigQueryì˜ ê°€ì¥ ì¤‘ìš”í•œ ê°œë…ì…ë‹ˆë‹¤. GA4, Amplitude ë°ì´í„°ëŠ” ëª¨ë‘ ì¤‘ì²© êµ¬ì¡°ì…ë‹ˆë‹¤.

#### 5-1. ARRAY ì–¸ë„¤ìŠ¤íŒ…

```sql
-- event_params ë°°ì—´ì—ì„œ íŠ¹ì • íŒŒë¼ë¯¸í„° ì¶”ì¶œ
SELECT
  event_name,
  user_id,
  timestamp,
  -- ë°°ì—´ì„ í–‰ìœ¼ë¡œ í¼ì¹¨
  param.key AS param_key,
  param.value.string_value,
  param.value.int_value
FROM `tracking-analytics.tracking_data.events`,
  UNNEST(event_params) AS param
WHERE DATE(timestamp) = '2025-01-15'
  AND event_name = 'product_view'
LIMIT 10;
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
event_name   | user_id  | timestamp           | param_key  | string_value | int_value
product_view | user_001 | 2025-01-15 10:00:00 | product_id | PROD-123    | NULL
product_view | user_001 | 2025-01-15 10:00:00 | price      | NULL        | 89000
```

#### 5-2. íŠ¹ì • íŒŒë¼ë¯¸í„°ë§Œ ì¶”ì¶œ (í—¬í¼ í•¨ìˆ˜)

```sql
-- íŒŒë¼ë¯¸í„° ê°’ ì¶”ì¶œ í•¨ìˆ˜
CREATE TEMP FUNCTION GET_PARAM_STRING(params ARRAY<STRUCT<key STRING, value STRUCT<string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64>>>, param_key STRING)
AS (
  (SELECT value.string_value FROM UNNEST(params) WHERE key = param_key)
);

CREATE TEMP FUNCTION GET_PARAM_INT(params ARRAY<STRUCT<key STRING, value STRUCT<string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64>>>, param_key STRING)
AS (
  (SELECT value.int_value FROM UNNEST(params) WHERE key = param_key)
);

-- ì‚¬ìš© ì˜ˆì‹œ
SELECT
  event_name,
  user_id,
  GET_PARAM_STRING(event_params, 'product_id') AS product_id,
  GET_PARAM_INT(event_params, 'price') AS price,
  GET_PARAM_STRING(event_params, 'category') AS category
FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) = '2025-01-15'
  AND event_name = 'product_view'
LIMIT 10;
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
event_name   | user_id  | product_id | price | category
product_view | user_001 | PROD-123   | 89000 | electronics
product_view | user_002 | PROD-456   | 45000 | fashion
```

---

### 6. GA4 ë°ì´í„° ë¶„ì„

#### 6-1. GA4 BigQuery Export êµ¬ì¡°

GA4 ë°ì´í„°ëŠ” `events_YYYYMMDD` í˜•ì‹ì˜ í…Œì´ë¸”ë¡œ ìë™ ì €ì¥ë©ë‹ˆë‹¤:

```sql
-- GA4 í…Œì´ë¸” êµ¬ì¡° í™•ì¸
SELECT
  event_name,
  event_timestamp,
  user_pseudo_id,
  device.category AS device_category,
  geo.country AS country,
  traffic_source.source AS source,
  traffic_source.medium AS medium
FROM `project-id.analytics_123456789.events_20250115`
LIMIT 10;
```

#### 6-2. GA4 ì´ë²¤íŠ¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ

```sql
-- GA4 event_paramsì—ì„œ ê°’ ì¶”ì¶œ
SELECT
  event_name,
  user_pseudo_id,
  TIMESTAMP_MICROS(event_timestamp) AS event_time,

  -- íŒŒë¼ë¯¸í„° ì¶”ì¶œ
  (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_url,
  (SELECT value.int_value FROM UNNEST(event_params) WHERE key = 'engagement_time_msec') AS engagement_time,
  (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'session_engaged') AS session_engaged

FROM `project-id.analytics_123456789.events_*`
WHERE _TABLE_SUFFIX BETWEEN '20250101' AND '20250131'
  AND event_name = 'page_view'
LIMIT 100;
```

#### 6-3. GA4 ì´ì»¤ë¨¸ìŠ¤ ì´ë²¤íŠ¸ ë¶„ì„

```sql
-- GA4 êµ¬ë§¤ ì´ë²¤íŠ¸ ë¶„ì„
SELECT
  DATE(TIMESTAMP_MICROS(event_timestamp)) AS purchase_date,
  user_pseudo_id,

  -- ê±°ë˜ ì •ë³´
  (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'transaction_id') AS transaction_id,
  (SELECT value.double_value FROM UNNEST(event_params) WHERE key = 'value') AS revenue,
  (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'currency') AS currency,

  -- ìƒí’ˆ ì •ë³´ (items ë°°ì—´)
  items.item_id,
  items.item_name,
  items.price,
  items.quantity

FROM `project-id.analytics_123456789.events_*`,
  UNNEST(items) AS items
WHERE _TABLE_SUFFIX BETWEEN '20250101' AND '20250131'
  AND event_name = 'purchase'
ORDER BY purchase_date DESC;
```

---

### 7. í¼ë„ ë¶„ì„

#### 7-1. ê¸°ë³¸ í¼ë„

```sql
-- êµ¬ë§¤ í¼ë„ ì „í™˜ìœ¨
WITH funnel_steps AS (
  SELECT
    DATE(timestamp) AS date,
    user_id,
    COUNTIF(event_name = 'product_view') AS step1_view,
    COUNTIF(event_name = 'add_to_cart') AS step2_cart,
    COUNTIF(event_name = 'checkout_start') AS step3_checkout,
    COUNTIF(event_name = 'purchase_complete') AS step4_purchase
  FROM `tracking-analytics.tracking_data.events`
  WHERE DATE(timestamp) BETWEEN '2025-01-01' AND '2025-01-31'
  GROUP BY date, user_id
)
SELECT
  date,
  COUNT(DISTINCT IF(step1_view > 0, user_id, NULL)) AS users_viewed,
  COUNT(DISTINCT IF(step2_cart > 0, user_id, NULL)) AS users_carted,
  COUNT(DISTINCT IF(step3_checkout > 0, user_id, NULL)) AS users_checkout,
  COUNT(DISTINCT IF(step4_purchase > 0, user_id, NULL)) AS users_purchased,

  -- ì „í™˜ìœ¨
  ROUND(COUNT(DISTINCT IF(step2_cart > 0, user_id, NULL)) / COUNT(DISTINCT IF(step1_view > 0, user_id, NULL)) * 100, 2) AS view_to_cart_rate,
  ROUND(COUNT(DISTINCT IF(step4_purchase > 0, user_id, NULL)) / COUNT(DISTINCT IF(step1_view > 0, user_id, NULL)) * 100, 2) AS overall_conversion_rate
FROM funnel_steps
GROUP BY date
ORDER BY date DESC;
```

#### 7-2. ìˆœì°¨ í¼ë„ (ì‹œê°„ ìˆœì„œ ê³ ë ¤)

```sql
-- ì‚¬ìš©ìê°€ ìˆœì„œëŒ€ë¡œ í¼ë„ì„ í†µê³¼í–ˆëŠ”ì§€ í™•ì¸
WITH user_events AS (
  SELECT
    user_id,
    event_name,
    timestamp,
    ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp) AS event_order
  FROM `tracking-analytics.tracking_data.events`
  WHERE DATE(timestamp) BETWEEN '2025-01-01' AND '2025-01-31'
    AND event_name IN ('product_view', 'add_to_cart', 'checkout_start', 'purchase_complete')
),
funnel_completion AS (
  SELECT
    user_id,
    MAX(CASE WHEN event_name = 'product_view' THEN event_order END) AS view_order,
    MAX(CASE WHEN event_name = 'add_to_cart' THEN event_order END) AS cart_order,
    MAX(CASE WHEN event_name = 'checkout_start' THEN event_order END) AS checkout_order,
    MAX(CASE WHEN event_name = 'purchase_complete' THEN event_order END) AS purchase_order
  FROM user_events
  GROUP BY user_id
)
SELECT
  COUNT(DISTINCT user_id) AS total_users,
  COUNT(DISTINCT IF(view_order IS NOT NULL, user_id, NULL)) AS reached_view,
  COUNT(DISTINCT IF(cart_order > view_order, user_id, NULL)) AS reached_cart,
  COUNT(DISTINCT IF(checkout_order > cart_order, user_id, NULL)) AS reached_checkout,
  COUNT(DISTINCT IF(purchase_order > checkout_order, user_id, NULL)) AS completed_purchase
FROM funnel_completion;
```

---

### 8. ì½”í˜¸íŠ¸ ë¶„ì„

#### 8-1. ì›”ë³„ ì½”í˜¸íŠ¸ ë¦¬í…ì…˜

```sql
-- ê°€ì… ì›”ë³„ ë¦¬í…ì…˜
WITH user_cohorts AS (
  SELECT
    user_id,
    DATE_TRUNC(MIN(DATE(timestamp)), MONTH) AS cohort_month
  FROM `tracking-analytics.tracking_data.events`
  WHERE DATE(timestamp) >= '2024-01-01'
  GROUP BY user_id
),
user_activity AS (
  SELECT
    uc.user_id,
    uc.cohort_month,
    DATE_TRUNC(DATE(e.timestamp), MONTH) AS activity_month
  FROM user_cohorts uc
  JOIN `tracking-analytics.tracking_data.events` e
    ON uc.user_id = e.user_id
  WHERE DATE(e.timestamp) >= '2024-01-01'
  GROUP BY uc.user_id, uc.cohort_month, activity_month
)
SELECT
  cohort_month,
  activity_month,
  COUNT(DISTINCT user_id) AS active_users,
  DATE_DIFF(activity_month, cohort_month, MONTH) AS months_since_cohort
FROM user_activity
GROUP BY cohort_month, activity_month
ORDER BY cohort_month, activity_month;
```

#### 8-2. ë¦¬í…ì…˜ìœ¨ í”¼ë²—

```sql
-- ë¦¬í…ì…˜ìœ¨ì„ í”¼ë²— í…Œì´ë¸”ë¡œ í‘œì‹œ
WITH cohort_retention AS (
  -- (ìœ„ì˜ ì¿¼ë¦¬ ê²°ê³¼ ì‚¬ìš©)
  SELECT
    cohort_month,
    months_since_cohort,
    active_users,
    FIRST_VALUE(active_users) OVER (PARTITION BY cohort_month ORDER BY months_since_cohort) AS cohort_size
  FROM (
    -- ì½”í˜¸íŠ¸ ë°ì´í„°
    SELECT
      cohort_month,
      activity_month,
      COUNT(DISTINCT user_id) AS active_users,
      DATE_DIFF(activity_month, cohort_month, MONTH) AS months_since_cohort
    FROM user_activity
    GROUP BY cohort_month, activity_month
  )
)
SELECT
  cohort_month,
  cohort_size,
  ROUND(MAX(IF(months_since_cohort = 0, active_users / cohort_size * 100, NULL)), 2) AS month_0,
  ROUND(MAX(IF(months_since_cohort = 1, active_users / cohort_size * 100, NULL)), 2) AS month_1,
  ROUND(MAX(IF(months_since_cohort = 2, active_users / cohort_size * 100, NULL)), 2) AS month_2,
  ROUND(MAX(IF(months_since_cohort = 3, active_users / cohort_size * 100, NULL)), 2) AS month_3
FROM cohort_retention
GROUP BY cohort_month, cohort_size
ORDER BY cohort_month;
```

---

### 9. ê³ ê¸‰ ë¶„ì„

#### 9-1. ì‚¬ìš©ì ì„¸ì…˜ ì¬êµ¬ì„±

```sql
-- 30ë¶„ ê¸°ì¤€ ì„¸ì…˜ êµ¬ë¶„
WITH events_with_session AS (
  SELECT
    user_id,
    event_name,
    timestamp,
    -- ì´ì „ ì´ë²¤íŠ¸ì™€ì˜ ì‹œê°„ ì°¨ì´
    TIMESTAMP_DIFF(
      timestamp,
      LAG(timestamp) OVER (PARTITION BY user_id ORDER BY timestamp),
      MINUTE
    ) AS minutes_since_last_event,
    -- 30ë¶„ ì´ìƒ ì°¨ì´ë‚˜ë©´ ìƒˆ ì„¸ì…˜
    SUM(
      IF(
        TIMESTAMP_DIFF(
          timestamp,
          LAG(timestamp) OVER (PARTITION BY user_id ORDER BY timestamp),
          MINUTE
        ) > 30 OR LAG(timestamp) OVER (PARTITION BY user_id ORDER BY timestamp) IS NULL,
        1,
        0
      )
    ) OVER (PARTITION BY user_id ORDER BY timestamp) AS session_number
  FROM `tracking-analytics.tracking_data.events`
  WHERE DATE(timestamp) BETWEEN '2025-01-01' AND '2025-01-31'
)
SELECT
  user_id,
  session_number,
  MIN(timestamp) AS session_start,
  MAX(timestamp) AS session_end,
  COUNT(*) AS events_in_session,
  TIMESTAMP_DIFF(MAX(timestamp), MIN(timestamp), MINUTE) AS session_duration_minutes
FROM events_with_session
GROUP BY user_id, session_number
ORDER BY user_id, session_number;
```

#### 9-2. êµ¬ë§¤ ì˜ˆì¸¡ (ë¨¸ì‹ ëŸ¬ë‹ ì¤€ë¹„)

```sql
-- êµ¬ë§¤ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì‚¬ìš©ì íŠ¹ì„± ì¶”ì¶œ
WITH user_features AS (
  SELECT
    user_id,
    -- í™œë™ ì§€í‘œ
    COUNT(*) AS total_events,
    COUNT(DISTINCT DATE(timestamp)) AS active_days,
    COUNT(DISTINCT IF(event_name = 'product_view', 1, NULL)) AS product_views,
    COUNT(DISTINCT IF(event_name = 'add_to_cart', 1, NULL)) AS cart_adds,

    -- êµ¬ë§¤ ì—¬ë¶€
    MAX(IF(event_name = 'purchase_complete', 1, 0)) AS has_purchased,

    -- ì‹œê°„ íŒ¨í„´
    AVG(EXTRACT(HOUR FROM timestamp)) AS avg_active_hour,

    -- í”Œë«í¼
    APPROX_TOP_COUNT(platform, 1)[OFFSET(0)].value AS primary_platform
  FROM `tracking-analytics.tracking_data.events`
  WHERE DATE(timestamp) BETWEEN '2025-01-01' AND '2025-01-31'
  GROUP BY user_id
)
SELECT
  has_purchased,
  COUNT(*) AS user_count,
  AVG(total_events) AS avg_events,
  AVG(active_days) AS avg_active_days,
  AVG(product_views) AS avg_product_views,
  AVG(cart_adds) AS avg_cart_adds
FROM user_features
GROUP BY has_purchased;
```

---

### 10. ì„±ëŠ¥ ìµœì í™”

#### 10-1. íŒŒí‹°ì…˜ í”„ë£¨ë‹

```sql
-- âŒ ë‚˜ìœ ì˜ˆ: ì „ì²´ í…Œì´ë¸” ìŠ¤ìº” (ë¹„ìš© ë†’ìŒ)
SELECT COUNT(*)
FROM `tracking-analytics.tracking_data.events`
WHERE event_name = 'purchase_complete';

-- âœ… ì¢‹ì€ ì˜ˆ: íŒŒí‹°ì…˜ í•„í„° ì‚¬ìš© (ë¹„ìš© ì ˆê°)
SELECT COUNT(*)
FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) BETWEEN '2025-01-01' AND '2025-01-31'
  AND event_name = 'purchase_complete';
```

#### 10-2. í´ëŸ¬ìŠ¤í„°ë§ í™œìš©

```sql
-- í´ëŸ¬ìŠ¤í„° ì»¬ëŸ¼ìœ¼ë¡œ í•„í„°ë§í•˜ë©´ ì„±ëŠ¥ í–¥ìƒ
SELECT *
FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) = '2025-01-15'
  AND event_name = 'purchase_complete'  -- í´ëŸ¬ìŠ¤í„° ì»¬ëŸ¼
  AND user_id = 'user_001';  -- í´ëŸ¬ìŠ¤í„° ì»¬ëŸ¼
```

#### 10-3. ì¿¼ë¦¬ ê²°ê³¼ ìºì‹±

```sql
-- ë™ì¼í•œ ì¿¼ë¦¬ë¥¼ 24ì‹œê°„ ë‚´ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ìºì‹œ ì‚¬ìš© (ë¬´ë£Œ)
-- ìºì‹œ ì‚¬ìš© ì•ˆ í•¨ ì˜µì…˜
SELECT *
FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) = '2025-01-15'
OPTIONS(use_cache=false);
```

#### 10-4. êµ¬ì²´í™”ëœ ë·° (Materialized View)

```sql
-- ìì£¼ ì‚¬ìš©í•˜ëŠ” ì§‘ê³„ë¥¼ ë¯¸ë¦¬ ê³„ì‚°
CREATE MATERIALIZED VIEW `tracking-analytics.tracking_data.daily_summary`
PARTITION BY summary_date
AS
SELECT
  DATE(timestamp) AS summary_date,
  event_name,
  COUNT(*) AS event_count,
  COUNT(DISTINCT user_id) AS unique_users
FROM `tracking-analytics.tracking_data.events`
GROUP BY summary_date, event_name;

-- ì‚¬ìš© (í›¨ì”¬ ë¹ ë¦„)
SELECT * FROM `tracking-analytics.tracking_data.daily_summary`
WHERE summary_date BETWEEN '2025-01-01' AND '2025-01-31';
```

---

### 11. ë¹„ìš© ê´€ë¦¬

#### 11-1. ì¿¼ë¦¬ ë¹„ìš© í™•ì¸

```sql
-- ì¿¼ë¦¬ ì‹¤í–‰ ì „ ìŠ¤ìº”í•  ë°ì´í„°ëŸ‰ í™•ì¸
-- ì›¹ ì½˜ì†”ì—ì„œ ì¿¼ë¦¬ ì…ë ¥ ì‹œ ìš°ì¸¡ ìƒë‹¨ì— "This query will process X MB" í‘œì‹œ

-- Pythonìœ¼ë¡œ ì˜ˆìƒ ë¹„ìš© ê³„ì‚°
from google.cloud import bigquery

client = bigquery.Client()
query = """
  SELECT COUNT(*) FROM `tracking-analytics.tracking_data.events`
  WHERE DATE(timestamp) BETWEEN '2025-01-01' AND '2025-01-31'
"""

job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)
query_job = client.query(query, job_config=job_config)

print(f"ìŠ¤ìº”í•  ë°ì´í„°: {query_job.total_bytes_processed / 1024**3:.2f} GB")
print(f"ì˜ˆìƒ ë¹„ìš©: ${query_job.total_bytes_processed / 1024**4 * 5:.4f}")  # $5/TB
```

#### 11-2. ë¹„ìš© ì ˆê° íŒ

**1) í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ**
```sql
-- âŒ ë‚˜ìœ ì˜ˆ
SELECT * FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) = '2025-01-15';

-- âœ… ì¢‹ì€ ì˜ˆ
SELECT event_name, user_id, timestamp
FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) = '2025-01-15';
```

**2) LIMITëŠ” ë¹„ìš© ì ˆê° ì•ˆ ë¨**
```sql
-- âŒ LIMIT 10ì´ì–´ë„ ì „ì²´ í…Œì´ë¸” ìŠ¤ìº”
SELECT * FROM `tracking-analytics.tracking_data.events` LIMIT 10;

-- âœ… íŒŒí‹°ì…˜ í•„í„°ë¡œ ìŠ¤ìº” ë²”ìœ„ ì œí•œ
SELECT * FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) = '2025-01-15'
LIMIT 10;
```

**3) íŒŒí‹°ì…˜/í´ëŸ¬ìŠ¤í„° í™œìš©**
```sql
-- âœ… íŒŒí‹°ì…˜ + í´ëŸ¬ìŠ¤í„° ì»¬ëŸ¼ ì‚¬ìš©
SELECT *
FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) = '2025-01-15'  -- íŒŒí‹°ì…˜
  AND event_name = 'purchase_complete'  -- í´ëŸ¬ìŠ¤í„°
  AND user_id = 'user_001';  -- í´ëŸ¬ìŠ¤í„°
```

---

### 12. Python ì—°ë™

#### 12-1. ë°ì´í„° ì¡°íšŒ

```python
from google.cloud import bigquery
import pandas as pd

client = bigquery.Client(project='tracking-analytics')

query = """
  SELECT
    DATE(timestamp) AS date,
    event_name,
    COUNT(*) AS event_count
  FROM `tracking-analytics.tracking_data.events`
  WHERE DATE(timestamp) BETWEEN '2025-01-01' AND '2025-01-31'
  GROUP BY date, event_name
  ORDER BY date DESC, event_count DESC
"""

# DataFrameìœ¼ë¡œ ì¡°íšŒ
df = client.query(query).to_dataframe()
print(df.head())

# ë˜ëŠ” pandas-gbq ì‚¬ìš©
import pandas_gbq

df = pandas_gbq.read_gbq(query, project_id='tracking-analytics')
print(df.head())
```

#### 12-2. ë°ì´í„° ì €ì¥

```python
# DataFrameì„ BigQueryì— ì €ì¥
import pandas as pd
from google.cloud import bigquery

client = bigquery.Client()

# ìƒ˜í”Œ ë°ì´í„°
df = pd.DataFrame({
    'event_id': ['evt_001', 'evt_002'],
    'event_name': ['product_view', 'add_to_cart'],
    'user_id': ['user_001', 'user_001'],
    'timestamp': pd.to_datetime(['2025-01-15 10:00:00', '2025-01-15 10:02:30'])
})

# ì €ì¥
table_id = 'tracking-analytics.tracking_data.events'
job_config = bigquery.LoadJobConfig(
    write_disposition='WRITE_APPEND',  # ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€
    # write_disposition='WRITE_TRUNCATE',  # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì…ë ¥
)

job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
job.result()

print(f"{len(df)} í–‰ ì €ì¥ ì™„ë£Œ")
```

#### 12-3. ìŠ¤ì¼€ì¤„ëœ ì¿¼ë¦¬ (ìë™í™”)

```python
from google.cloud import bigquery_datatransfer

transfer_client = bigquery_datatransfer.DataTransferServiceClient()

project_id = 'tracking-analytics'
display_name = 'Daily Summary'

# ë§¤ì¼ ì˜¤ì „ 9ì‹œ ì‹¤í–‰
schedule = 'every day 09:00'

transfer_config = bigquery_datatransfer.TransferConfig(
    destination_dataset_id='tracking_data',
    display_name=display_name,
    data_source_id='scheduled_query',
    params={
        'query': """
          INSERT INTO `tracking-analytics.tracking_data.daily_summary`
          SELECT
            CURRENT_DATE() AS summary_date,
            event_name,
            COUNT(*) AS event_count,
            COUNT(DISTINCT user_id) AS unique_users
          FROM `tracking-analytics.tracking_data.events`
          WHERE DATE(timestamp) = CURRENT_DATE() - 1
          GROUP BY event_name
        """,
    },
    schedule=schedule,
)

parent = transfer_client.common_project_path(project_id)
transfer_config = transfer_client.create_transfer_config(
    parent=parent, transfer_config=transfer_config
)

print(f"ìŠ¤ì¼€ì¤„ ìƒì„± ì™„ë£Œ: {transfer_config.name}")
```

---

### 13. ì‹¤ë¬´ í™œìš© ì˜ˆì‹œ

#### ì˜ˆì‹œ 1: ì¼ì¼ ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬

```sql
-- ì–´ì œ ì£¼ìš” ì§€í‘œ
DECLARE yesterday DATE DEFAULT DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY);

SELECT
  'DAU' AS metric,
  COUNT(DISTINCT user_id) AS value
FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) = yesterday

UNION ALL

SELECT
  'Total Events' AS metric,
  COUNT(*) AS value
FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) = yesterday

UNION ALL

SELECT
  'Purchasers' AS metric,
  COUNT(DISTINCT user_id) AS value
FROM `tracking-analytics.tracking_data.events`
WHERE DATE(timestamp) = yesterday
  AND event_name = 'purchase_complete'

UNION ALL

SELECT
  'Revenue' AS metric,
  CAST(SUM(total_amount) AS INT64) AS value
FROM `tracking-analytics.tracking_data.purchases`
WHERE DATE(timestamp) = yesterday;
```

#### ì˜ˆì‹œ 2: ì£¼ê°„ ë¦¬í¬íŠ¸

```sql
-- ìµœê·¼ 7ì¼ vs ì´ì „ 7ì¼ ë¹„êµ
WITH current_week AS (
  SELECT
    COUNT(DISTINCT user_id) AS dau,
    COUNT(*) AS events,
    COUNT(DISTINCT IF(event_name = 'purchase_complete', user_id, NULL)) AS purchasers
  FROM `tracking-analytics.tracking_data.events`
  WHERE DATE(timestamp) BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) AND CURRENT_DATE()
),
previous_week AS (
  SELECT
    COUNT(DISTINCT user_id) AS dau,
    COUNT(*) AS events,
    COUNT(DISTINCT IF(event_name = 'purchase_complete', user_id, NULL)) AS purchasers
  FROM `tracking-analytics.tracking_data.events`
  WHERE DATE(timestamp) BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 14 DAY) AND DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
)
SELECT
  'DAU' AS metric,
  cw.dau AS current_week,
  pw.dau AS previous_week,
  ROUND((cw.dau - pw.dau) / pw.dau * 100, 2) AS change_pct
FROM current_week cw, previous_week pw

UNION ALL

SELECT
  'Events' AS metric,
  cw.events,
  pw.events,
  ROUND((cw.events - pw.events) / pw.events * 100, 2)
FROM current_week cw, previous_week pw

UNION ALL

SELECT
  'Purchasers' AS metric,
  cw.purchasers,
  pw.purchasers,
  ROUND((cw.purchasers - pw.purchasers) / pw.purchasers * 100, 2)
FROM current_week cw, previous_week pw;
```

---

### 14. ì‹¤ë¬´ íŒ

#### íŒ 1: WITH ì ˆ í™œìš©

ë³µì¡í•œ ì¿¼ë¦¬ëŠ” WITH ì ˆë¡œ ë‚˜ëˆ„ë©´ ê°€ë…ì„±ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤:

```sql
WITH base_events AS (
  SELECT * FROM `tracking-analytics.tracking_data.events`
  WHERE DATE(timestamp) BETWEEN '2025-01-01' AND '2025-01-31'
),
user_stats AS (
  SELECT
    user_id,
    COUNT(*) AS total_events,
    MAX(IF(event_name = 'purchase_complete', 1, 0)) AS has_purchased
  FROM base_events
  GROUP BY user_id
)
SELECT
  has_purchased,
  COUNT(*) AS user_count,
  AVG(total_events) AS avg_events
FROM user_stats
GROUP BY has_purchased;
```

#### íŒ 2: ë‚ ì§œ í•¨ìˆ˜ ë§ˆìŠ¤í„°

```sql
-- ìœ ìš©í•œ ë‚ ì§œ í•¨ìˆ˜ë“¤
SELECT
  CURRENT_DATE() AS today,
  DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) AS week_ago,
  DATE_TRUNC(CURRENT_DATE(), MONTH) AS month_start,
  LAST_DAY(CURRENT_DATE()) AS month_end,
  EXTRACT(DAYOFWEEK FROM CURRENT_DATE()) AS day_of_week,
  FORMAT_DATE('%Y-%m-%d', CURRENT_DATE()) AS formatted_date;
```

#### íŒ 3: ì¿¼ë¦¬ í…œí”Œë¦¿ ì €ì¥

ìì£¼ ì‚¬ìš©í•˜ëŠ” ì¿¼ë¦¬ëŠ” ì €ì¥í•´ë‘ì„¸ìš”:

```sql
-- ì›¹ ì½˜ì†”ì—ì„œ "Save Query" í´ë¦­
-- ë˜ëŠ” Saved Queriesì— ì €ì¥
```

---

ì´ê²ƒìœ¼ë¡œ ë¶€ë¡ C-3ì„ ë§ˆì¹©ë‹ˆë‹¤. ì´ì œ BigQueryë¡œ ëŒ€ìš©ëŸ‰ íŠ¸ë˜í‚¹ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

---

## ë¶€ë¡ D: ì‹¤ìŠµ í”„ë¡œì íŠ¸ - ì´ì»¤ë¨¸ìŠ¤ ë°ì´í„° ë¶„ì„

> **ëª©í‘œ:** ì‹¤ì œ ì´ì»¤ë¨¸ìŠ¤ ë°ì´í„°ì…‹ìœ¼ë¡œ íŠ¸ë˜í‚¹ í”Œëœ ì„¤ê³„ë¶€í„° ë¶„ì„ê¹Œì§€ ì „ ê³¼ì •ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.

### 1. í”„ë¡œì íŠ¸ ê°œìš”

#### 1-1. í”„ë¡œì íŠ¸ ë°°ê²½

**íšŒì‚¬:** "ShopKorea" ì˜¨ë¼ì¸ ì‡¼í•‘ëª°
**ìƒí™©:** ì‹ ê·œ ëŸ°ì¹­ 1ê°œì›” í›„, ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì • í•„ìš”
**ë°ì´í„°:** 2025ë…„ 1ì›” í•œ ë‹¬ê°„ ìˆ˜ì§‘ëœ ì‚¬ìš©ì í–‰ë™ ë°ì´í„°

**í•µì‹¬ ì§ˆë¬¸:**
1. êµ¬ë§¤ ì „í™˜ìœ¨ì€ ì–¼ë§ˆë‚˜ ë˜ëŠ”ê°€?
2. ì–´ëŠ ë‹¨ê³„ì—ì„œ ê°€ì¥ ë§ì´ ì´íƒˆí•˜ëŠ”ê°€?
3. êµ¬ë§¤ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì‚¬ìš©ìì˜ íŠ¹ì„±ì€?
4. ë§¤ì¶œì„ ëŠ˜ë¦¬ë ¤ë©´ ì–´ë””ì— ì§‘ì¤‘í•´ì•¼ í•˜ëŠ”ê°€?

#### 1-2. ë°ì´í„°ì…‹ êµ¬ì¡°

**ì´ë²¤íŠ¸ ì¢…ë¥˜:**
- `app_open`: ì•± ì‹¤í–‰
- `product_view`: ìƒí’ˆ ì¡°íšŒ
- `add_to_cart`: ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸°
- `checkout_start`: ê²°ì œ ì‹œì‘
- `purchase_complete`: êµ¬ë§¤ ì™„ë£Œ

**ë°ì´í„° ê·œëª¨:**
- ê¸°ê°„: 2025-01-01 ~ 2025-01-31 (31ì¼)
- ì‚¬ìš©ì ìˆ˜: 500ëª…
- ì´ ì´ë²¤íŠ¸: ì•½ 10,000ê±´
- êµ¬ë§¤ ê±´ìˆ˜: ì•½ 50ê±´

---

### 2. ìƒ˜í”Œ ë°ì´í„° ìƒì„±

#### 2-1. Pythonìœ¼ë¡œ ë°ì´í„° ìƒì„±

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì´ì»¤ë¨¸ìŠ¤ ìƒ˜í”Œ ë°ì´í„° ìƒì„±ê¸°
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import random

# ì‹œë“œ ê³ ì • (ì¬í˜„ ê°€ëŠ¥í•˜ë„ë¡)
np.random.seed(42)
random.seed(42)

# ì„¤ì •
NUM_USERS = 500
START_DATE = datetime(2025, 1, 1)
END_DATE = datetime(2025, 1, 31)

# ìƒí’ˆ ëª©ë¡
PRODUCTS = [
    {'id': 'PROD-001', 'name': 'ë…¸íŠ¸ë¶', 'price': 1200000, 'category': 'electronics'},
    {'id': 'PROD-002', 'name': 'ë¬´ì„  ì´ì–´í°', 'price': 89000, 'category': 'electronics'},
    {'id': 'PROD-003', 'name': 'ìŠ¤ë§ˆíŠ¸ì›Œì¹˜', 'price': 350000, 'category': 'electronics'},
    {'id': 'PROD-004', 'name': 'ë°±íŒ©', 'price': 65000, 'category': 'fashion'},
    {'id': 'PROD-005', 'name': 'ìš´ë™í™”', 'price': 120000, 'category': 'fashion'},
    {'id': 'PROD-006', 'name': 'í‹°ì…”ì¸ ', 'price': 29000, 'category': 'fashion'},
    {'id': 'PROD-007', 'name': 'ì±…ìƒ', 'price': 180000, 'category': 'furniture'},
    {'id': 'PROD-008', 'name': 'ì˜ì', 'price': 220000, 'category': 'furniture'},
]

PLATFORMS = ['iOS', 'Android', 'Web']
DEVICE_TYPES = ['mobile', 'mobile', 'desktop']

def generate_user_id(i):
    """ì‚¬ìš©ì ID ìƒì„±"""
    return f"user_{i:04d}"

def random_timestamp(start, end):
    """ëœë¤ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±"""
    delta = end - start
    random_seconds = random.randint(0, int(delta.total_seconds()))
    return start + timedelta(seconds=random_seconds)

def generate_events():
    """ì´ë²¤íŠ¸ ë°ì´í„° ìƒì„±"""
    events = []

    for user_idx in range(NUM_USERS):
        user_id = generate_user_id(user_idx)

        # ì‚¬ìš©ì íŠ¹ì„±
        platform = random.choice(PLATFORMS)
        device_type = DEVICE_TYPES[PLATFORMS.index(platform)]

        # í™œë™ ìˆ˜ì¤€ (20%ëŠ” ê³ í™œë™, 80%ëŠ” ì €í™œë™)
        is_active = random.random() < 0.2
        num_sessions = random.randint(3, 10) if is_active else random.randint(1, 3)

        for session_idx in range(num_sessions):
            session_id = f"sess_{user_id}_{session_idx}"
            session_start = random_timestamp(START_DATE, END_DATE)

            # ì•± ì‹¤í–‰
            events.append({
                'event_id': f"evt_{len(events):06d}",
                'event_name': 'app_open',
                'user_id': user_id,
                'session_id': session_id,
                'timestamp': session_start,
                'platform': platform,
                'device_type': device_type,
            })

            # ìƒí’ˆ ì¡°íšŒ (1-5ê°œ)
            num_views = random.randint(1, 5)
            current_time = session_start

            for view_idx in range(num_views):
                current_time += timedelta(seconds=random.randint(10, 120))
                product = random.choice(PRODUCTS)

                events.append({
                    'event_id': f"evt_{len(events):06d}",
                    'event_name': 'product_view',
                    'user_id': user_id,
                    'session_id': session_id,
                    'timestamp': current_time,
                    'platform': platform,
                    'device_type': device_type,
                    'product_id': product['id'],
                    'product_name': product['name'],
                    'price': product['price'],
                    'category': product['category'],
                })

                # 30% í™•ë¥ ë¡œ ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸°
                if random.random() < 0.3:
                    current_time += timedelta(seconds=random.randint(5, 60))
                    events.append({
                        'event_id': f"evt_{len(events):06d}",
                        'event_name': 'add_to_cart',
                        'user_id': user_id,
                        'session_id': session_id,
                        'timestamp': current_time,
                        'platform': platform,
                        'device_type': device_type,
                        'product_id': product['id'],
                        'product_name': product['name'],
                        'price': product['price'],
                        'category': product['category'],
                    })

                    # 50% í™•ë¥ ë¡œ ê²°ì œ ì‹œì‘
                    if random.random() < 0.5:
                        current_time += timedelta(seconds=random.randint(10, 180))
                        events.append({
                            'event_id': f"evt_{len(events):06d}",
                            'event_name': 'checkout_start',
                            'user_id': user_id,
                            'session_id': session_id,
                            'timestamp': current_time,
                            'platform': platform,
                            'device_type': device_type,
                            'product_id': product['id'],
                            'product_name': product['name'],
                            'price': product['price'],
                            'category': product['category'],
                        })

                        # 60% í™•ë¥ ë¡œ êµ¬ë§¤ ì™„ë£Œ
                        if random.random() < 0.6:
                            current_time += timedelta(seconds=random.randint(30, 300))
                            payment_method = random.choice(['card', 'kakaopay', 'naverpay'])

                            events.append({
                                'event_id': f"evt_{len(events):06d}",
                                'event_name': 'purchase_complete',
                                'user_id': user_id,
                                'session_id': session_id,
                                'timestamp': current_time,
                                'platform': platform,
                                'device_type': device_type,
                                'product_id': product['id'],
                                'product_name': product['name'],
                                'price': product['price'],
                                'category': product['category'],
                                'payment_method': payment_method,
                                'order_id': f"ORD-2025-{len([e for e in events if e['event_name'] == 'purchase_complete']) + 1:05d}",
                            })

    return pd.DataFrame(events)

# ë°ì´í„° ìƒì„±
print("ì´ë²¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
df_events = generate_events()

# ê¸°ë³¸ í†µê³„
print(f"\n=== ë°ì´í„° ìƒì„± ì™„ë£Œ ===")
print(f"ì „ì²´ ì´ë²¤íŠ¸ ìˆ˜: {len(df_events):,}")
print(f"ì „ì²´ ì‚¬ìš©ì ìˆ˜: {df_events['user_id'].nunique():,}")
print(f"\nì´ë²¤íŠ¸ë³„ ë°œìƒ íšŸìˆ˜:")
print(df_events['event_name'].value_counts())

# CSVë¡œ ì €ì¥
df_events.to_csv('data/events.csv', index=False, encoding='utf-8-sig')
print(f"\nì €ì¥ ì™„ë£Œ: data/events.csv")

# JSONLë¡œ ì €ì¥
with open('data/events.jsonl', 'w', encoding='utf-8') as f:
    for _, row in df_events.iterrows():
        json.dump(row.to_dict(), f, ensure_ascii=False, default=str)
        f.write('\n')
print(f"ì €ì¥ ì™„ë£Œ: data/events.jsonl")

# Parquetë¡œ ì €ì¥
df_events['timestamp'] = pd.to_datetime(df_events['timestamp'])
df_events.to_parquet('data/events.parquet', index=False, compression='snappy')
print(f"ì €ì¥ ì™„ë£Œ: data/events.parquet")
```

**ì‹¤í–‰:**
```bash
python generate_sample_data.py
```

---

### 3. ë¯¸ì…˜ 1: í¼ë„ ì „í™˜ìœ¨ ë¶„ì„

#### ëª©í‘œ
ê° ë‹¨ê³„ë³„ ì „í™˜ìœ¨ì„ ê³„ì‚°í•˜ê³ , ê°€ì¥ í° ì´íƒˆ ì§€ì ì„ ì°¾ìœ¼ì„¸ìš”.

#### íŒíŠ¸
```python
# Pandasë¡œ í¼ë„ ë¶„ì„
import pandas as pd

df = pd.read_parquet('data/events.parquet')

funnel_steps = ['product_view', 'add_to_cart', 'checkout_start', 'purchase_complete']

# ê° ë‹¨ê³„ë³„ ì‚¬ìš©ì ìˆ˜
funnel_data = []
for step in funnel_steps:
    user_count = df[df['event_name'] == step]['user_id'].nunique()
    funnel_data.append({'step': step, 'users': user_count})

funnel_df = pd.DataFrame(funnel_data)
funnel_df['conversion_rate'] = (funnel_df['users'] / funnel_df['users'].iloc[0] * 100).round(2)

print(funnel_df)
```

#### ì •ë‹µ (ì ‘ê¸°)

<details>
<summary>ì •ë‹µ ë³´ê¸°</summary>

```python
import pandas as pd
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë“œ
df = pd.read_parquet('data/events.parquet')

# í¼ë„ ë‹¨ê³„
funnel_steps = ['product_view', 'add_to_cart', 'checkout_start', 'purchase_complete']

# ê° ë‹¨ê³„ë³„ ê³ ìœ  ì‚¬ìš©ì ìˆ˜
funnel_data = []
for step in funnel_steps:
    user_count = df[df['event_name'] == step]['user_id'].nunique()
    event_count = len(df[df['event_name'] == step])
    funnel_data.append({
        'step': step,
        'users': user_count,
        'events': event_count
    })

funnel_df = pd.DataFrame(funnel_data)

# ì „í™˜ìœ¨ ê³„ì‚°
funnel_df['overall_conversion'] = (funnel_df['users'] / funnel_df['users'].iloc[0] * 100).round(2)
funnel_df['step_conversion'] = (funnel_df['users'] / funnel_df['users'].shift(1) * 100).round(2)

print("=== í¼ë„ ë¶„ì„ ê²°ê³¼ ===")
print(funnel_df)

# ê°€ì¥ í° ì´íƒˆ ì§€ì 
funnel_df['drop_rate'] = 100 - funnel_df['step_conversion']
max_drop_idx = funnel_df['drop_rate'].idxmax()
if not pd.isna(funnel_df.loc[max_drop_idx, 'drop_rate']):
    print(f"\nê°€ì¥ í° ì´íƒˆ ì§€ì : {funnel_df.loc[max_drop_idx, 'step']} ({funnel_df.loc[max_drop_idx, 'drop_rate']:.2f}% ì´íƒˆ)")

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.barh(funnel_df['step'], funnel_df['users'], color='skyblue')
plt.xlabel('ì‚¬ìš©ì ìˆ˜')
plt.title('êµ¬ë§¤ í¼ë„ ë¶„ì„')
plt.tight_layout()
plt.savefig('output/funnel_analysis.png', dpi=300)
print("\nì‹œê°í™” ì €ì¥ ì™„ë£Œ: output/funnel_analysis.png")
```

**ì˜ˆìƒ ê²°ê³¼:**
```
               step  users  events  overall_conversion  step_conversion  drop_rate
0     product_view    487    2456              100.00              NaN        NaN
1     add_to_cart    215     312               44.15            44.15      55.85
2   checkout_start    108     118               22.18            50.23      49.77
3 purchase_complete     65      65               13.35            60.19      39.81

ê°€ì¥ í° ì´íƒˆ ì§€ì : add_to_cart (55.85% ì´íƒˆ)
```

**ì¸ì‚¬ì´íŠ¸:**
- ìƒí’ˆ ì¡°íšŒ í›„ ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸° ë‹¨ê³„ì—ì„œ ê°€ì¥ í° ì´íƒˆ (55.85%)
- ê°œì„  ë°©ì•ˆ: ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸° ìœ ë„ CTA ê°œì„ , í• ì¸ ì¿ í° ì œê³µ
</details>

---

### 4. ë¯¸ì…˜ 2: ê³ ê°€ì¹˜ ì‚¬ìš©ì ì°¾ê¸° (RFM ë¶„ì„)

#### ëª©í‘œ
RFM ë¶„ì„ìœ¼ë¡œ ê³ ê°€ì¹˜ ì‚¬ìš©ìë¥¼ ì°¾ê³ , ì„¸ê·¸ë¨¼íŠ¸ë³„ íŠ¹ì„±ì„ íŒŒì•…í•˜ì„¸ìš”.

#### íŒíŠ¸
```python
# êµ¬ë§¤ ì´ë²¤íŠ¸ë§Œ ì¶”ì¶œ
purchases = df[df['event_name'] == 'purchase_complete'].copy()

# RFM ê³„ì‚°
analysis_date = purchases['timestamp'].max()
rfm = purchases.groupby('user_id').agg({
    'timestamp': lambda x: (analysis_date - x.max()).days,  # Recency
    'event_name': 'count',  # Frequency
    'price': 'sum'  # Monetary
})
```

#### ì •ë‹µ (ì ‘ê¸°)

<details>
<summary>ì •ë‹µ ë³´ê¸°</summary>

```python
import pandas as pd

# ë°ì´í„° ë¡œë“œ
df = pd.read_parquet('data/events.parquet')

# êµ¬ë§¤ ì´ë²¤íŠ¸ë§Œ ì¶”ì¶œ
purchases = df[df['event_name'] == 'purchase_complete'].copy()

print(f"ì´ êµ¬ë§¤ ê±´ìˆ˜: {len(purchases)}")
print(f"êµ¬ë§¤ ì‚¬ìš©ì ìˆ˜: {purchases['user_id'].nunique()}")

# ë¶„ì„ ê¸°ì¤€ì¼
analysis_date = purchases['timestamp'].max()

# RFM ê³„ì‚°
rfm = purchases.groupby('user_id').agg({
    'timestamp': lambda x: (analysis_date - x.max()).days,  # Recency
    'event_name': 'count',  # Frequency
    'price': 'sum'  # Monetary
}).reset_index()

rfm.columns = ['user_id', 'recency', 'frequency', 'monetary']

# RFM ì ìˆ˜ (1-5ì )
rfm['r_score'] = pd.qcut(rfm['recency'], 5, labels=[5, 4, 3, 2, 1], duplicates='drop')
rfm['f_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 5, labels=[1, 2, 3, 4, 5], duplicates='drop')
rfm['m_score'] = pd.qcut(rfm['monetary'], 5, labels=[1, 2, 3, 4, 5], duplicates='drop')

# ì´ì 
rfm['rfm_score'] = rfm['r_score'].astype(int) + rfm['f_score'].astype(int) + rfm['m_score'].astype(int)

# ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜
def rfm_segment(score):
    if score >= 12:
        return 'Champions'
    elif score >= 9:
        return 'Loyal Customers'
    elif score >= 6:
        return 'Potential'
    else:
        return 'At Risk'

rfm['segment'] = rfm['rfm_score'].apply(rfm_segment)

# ì„¸ê·¸ë¨¼íŠ¸ë³„ í†µê³„
segment_stats = rfm.groupby('segment').agg({
    'user_id': 'count',
    'recency': 'mean',
    'frequency': 'mean',
    'monetary': 'mean'
}).round(2)

print("\n=== RFM ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ ===")
print(segment_stats)

# ìƒìœ„ 10ëª… ê³ ê°€ì¹˜ ê³ ê°
top_customers = rfm.nlargest(10, 'monetary')[['user_id', 'recency', 'frequency', 'monetary', 'segment']]
print("\n=== ìƒìœ„ 10ëª… ê³ ê°€ì¹˜ ê³ ê° ===")
print(top_customers)

# ê²°ê³¼ ì €ì¥
rfm.to_csv('output/rfm_analysis.csv', index=False, encoding='utf-8-sig')
print("\nê²°ê³¼ ì €ì¥ ì™„ë£Œ: output/rfm_analysis.csv")
```

**ì˜ˆìƒ ê²°ê³¼:**
```
=== RFM ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ ===
                  user_id  recency  frequency      monetary
segment
At Risk                15    18.53       1.00     156666.67
Champions              12     8.25       2.42     458333.33
Loyal Customers        18    12.11       1.67     298333.33
Potential              20    15.45       1.15     198500.00

=== ìƒìœ„ 10ëª… ê³ ê°€ì¹˜ ê³ ê° ===
      user_id  recency  frequency   monetary        segment
user_0234       5          3      1550000     Champions
user_0089       7          3      1420000     Champions
...
```

**ì¸ì‚¬ì´íŠ¸:**
- Champions ê³ ê°ì€ í‰ê·  2.42íšŒ êµ¬ë§¤, í‰ê·  458,333ì› ì§€ì¶œ
- At Risk ê³ ê°ì€ 18ì¼ ì´ìƒ ë¯¸ë°©ë¬¸ â†’ ë¦¬ë§ˆì¼€íŒ… ìº í˜ì¸ í•„ìš”
</details>

---

### 5. ë¯¸ì…˜ 3: í”Œë«í¼ë³„ ì„±ê³¼ ë¹„êµ

#### ëª©í‘œ
iOS, Android, Web í”Œë«í¼ë³„ë¡œ ì „í™˜ìœ¨ê³¼ ë§¤ì¶œì„ ë¹„êµí•˜ì„¸ìš”.

#### ì •ë‹µ (ì ‘ê¸°)

<details>
<summary>ì •ë‹µ ë³´ê¸°</summary>

```python
import pandas as pd

df = pd.read_parquet('data/events.parquet')

# í”Œë«í¼ë³„ í¼ë„
platforms = ['iOS', 'Android', 'Web']
results = []

for platform in platforms:
    platform_df = df[df['platform'] == platform]

    views = platform_df[platform_df['event_name'] == 'product_view']['user_id'].nunique()
    purchases = platform_df[platform_df['event_name'] == 'purchase_complete']['user_id'].nunique()

    # ë§¤ì¶œ
    revenue = platform_df[platform_df['event_name'] == 'purchase_complete']['price'].sum()

    # ì „í™˜ìœ¨
    conversion_rate = (purchases / views * 100) if views > 0 else 0

    results.append({
        'platform': platform,
        'views': views,
        'purchases': purchases,
        'revenue': revenue,
        'conversion_rate': round(conversion_rate, 2)
    })

result_df = pd.DataFrame(results)

print("=== í”Œë«í¼ë³„ ì„±ê³¼ ===")
print(result_df)

# ì‹œê°í™”
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ì „í™˜ìœ¨ ë¹„êµ
axes[0].bar(result_df['platform'], result_df['conversion_rate'], color=['#1f77b4', '#2ca02c', '#ff7f0e'])
axes[0].set_title('í”Œë«í¼ë³„ ì „í™˜ìœ¨')
axes[0].set_ylabel('ì „í™˜ìœ¨ (%)')

# ë§¤ì¶œ ë¹„êµ
axes[1].bar(result_df['platform'], result_df['revenue'] / 1000000, color=['#1f77b4', '#2ca02c', '#ff7f0e'])
axes[1].set_title('í”Œë«í¼ë³„ ë§¤ì¶œ')
axes[1].set_ylabel('ë§¤ì¶œ (ë°±ë§Œì›)')

plt.tight_layout()
plt.savefig('output/platform_comparison.png', dpi=300)
print("\nì‹œê°í™” ì €ì¥ ì™„ë£Œ: output/platform_comparison.png")
```

**ì¸ì‚¬ì´íŠ¸:**
- iOS ì‚¬ìš©ìì˜ ì „í™˜ìœ¨ì´ ê°€ì¥ ë†’ìŒ
- Webì€ ìœ ì…ì´ ë§ì§€ë§Œ ì „í™˜ìœ¨ì´ ë‚®ìŒ â†’ UX ê°œì„  í•„ìš”
</details>

---

### 6. ë¯¸ì…˜ 4: ì‹œê°„ëŒ€ë³„ êµ¬ë§¤ íŒ¨í„´ ë¶„ì„

#### ëª©í‘œ
ì–´ëŠ ì‹œê°„ëŒ€ì— êµ¬ë§¤ê°€ ê°€ì¥ ë§ì´ ë°œìƒí•˜ëŠ”ì§€ ë¶„ì„í•˜ì„¸ìš”.

#### ì •ë‹µ (ì ‘ê¸°)

<details>
<summary>ì •ë‹µ ë³´ê¸°</summary>

```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_parquet('data/events.parquet')

# êµ¬ë§¤ ì´ë²¤íŠ¸ë§Œ ì¶”ì¶œ
purchases = df[df['event_name'] == 'purchase_complete'].copy()

# ì‹œê°„ëŒ€ ì¶”ê°€
purchases['hour'] = purchases['timestamp'].dt.hour

# ì‹œê°„ëŒ€ë³„ êµ¬ë§¤ ê±´ìˆ˜
hourly_purchases = purchases.groupby('hour').size()

print("=== ì‹œê°„ëŒ€ë³„ êµ¬ë§¤ ê±´ìˆ˜ ===")
print(hourly_purchases)

# ì‹œê°í™”
plt.figure(figsize=(12, 6))
plt.plot(hourly_purchases.index, hourly_purchases.values, marker='o', linewidth=2, markersize=8)
plt.xlabel('ì‹œê°„ëŒ€ (0-23ì‹œ)')
plt.ylabel('êµ¬ë§¤ ê±´ìˆ˜')
plt.title('ì‹œê°„ëŒ€ë³„ êµ¬ë§¤ íŒ¨í„´')
plt.grid(True, alpha=0.3)
plt.xticks(range(0, 24))
plt.tight_layout()
plt.savefig('output/hourly_purchases.png', dpi=300)
print("\nì‹œê°í™” ì €ì¥ ì™„ë£Œ: output/hourly_purchases.png")

# í”¼í¬ ì‹œê°„ëŒ€
peak_hour = hourly_purchases.idxmax()
peak_count = hourly_purchases.max()
print(f"\ní”¼í¬ ì‹œê°„ëŒ€: {peak_hour}ì‹œ ({peak_count}ê±´)")
```

**ì¸ì‚¬ì´íŠ¸:**
- ì €ë… 8-10ì‹œì— êµ¬ë§¤ê°€ ì§‘ì¤‘ë¨
- ë§ˆì¼€íŒ… ìº í˜ì¸ì€ ì €ë… ì‹œê°„ëŒ€ íƒ€ê²ŸíŒ… ì¶”ì²œ
</details>

---

### 7. ë¯¸ì…˜ 5: SQLë¡œ ë°ì´í„° ë¶„ì„ (MySQL)

#### ëª©í‘œ
ìƒì„±í•œ ë°ì´í„°ë¥¼ MySQLì— ë„£ê³  SQLë¡œ ë¶„ì„í•˜ì„¸ìš”.

#### ë‹¨ê³„

**1) ë°ì´í„° ë¡œë“œ:**
```python
import pandas as pd
import mysql.connector

# ë°ì´í„° ì½ê¸°
df = pd.read_csv('data/events.csv')

# MySQL ì—°ê²°
conn = mysql.connector.connect(
    host='localhost',
    user='root',
    password='your_password',
    database='tracking_db'
)
cursor = conn.cursor()

# í…Œì´ë¸” ìƒì„± (ë¶€ë¡ C-2 ì°¸ê³ )
create_table_query = """
CREATE TABLE IF NOT EXISTS events (
    event_id VARCHAR(50) PRIMARY KEY,
    event_name VARCHAR(100) NOT NULL,
    user_id VARCHAR(100) NOT NULL,
    session_id VARCHAR(100),
    timestamp DATETIME NOT NULL,
    platform VARCHAR(50),
    device_type VARCHAR(50),
    product_id VARCHAR(50),
    product_name VARCHAR(255),
    price INT,
    category VARCHAR(100),
    payment_method VARCHAR(50),
    order_id VARCHAR(100),
    INDEX idx_event_name (event_name),
    INDEX idx_user_id (user_id),
    INDEX idx_timestamp (timestamp)
);
"""
cursor.execute(create_table_query)

# ë°ì´í„° ì‚½ì…
for _, row in df.iterrows():
    insert_query = """
    INSERT INTO events (event_id, event_name, user_id, session_id, timestamp, platform, device_type,
                        product_id, product_name, price, category, payment_method, order_id)
    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
    ON DUPLICATE KEY UPDATE event_id=event_id
    """
    cursor.execute(insert_query, (
        row['event_id'], row['event_name'], row['user_id'], row.get('session_id'),
        row['timestamp'], row.get('platform'), row.get('device_type'),
        row.get('product_id'), row.get('product_name'), row.get('price'),
        row.get('category'), row.get('payment_method'), row.get('order_id')
    ))

conn.commit()
print(f"{len(df)} í–‰ ì…ë ¥ ì™„ë£Œ")
cursor.close()
conn.close()
```

**2) SQL í¼ë„ ë¶„ì„:**
```sql
-- í¼ë„ ì „í™˜ìœ¨ ê³„ì‚°
SELECT
    COUNT(DISTINCT CASE WHEN event_name = 'product_view' THEN user_id END) AS product_views,
    COUNT(DISTINCT CASE WHEN event_name = 'add_to_cart' THEN user_id END) AS add_to_cart,
    COUNT(DISTINCT CASE WHEN event_name = 'checkout_start' THEN user_id END) AS checkout_start,
    COUNT(DISTINCT CASE WHEN event_name = 'purchase_complete' THEN user_id END) AS purchases,
    ROUND(
        COUNT(DISTINCT CASE WHEN event_name = 'purchase_complete' THEN user_id END) /
        COUNT(DISTINCT CASE WHEN event_name = 'product_view' THEN user_id END) * 100,
        2
    ) AS conversion_rate
FROM events;
```

**3) ìƒí’ˆë³„ ë§¤ì¶œ:**
```sql
-- ìƒí’ˆë³„ ë§¤ì¶œ ìˆœìœ„
SELECT
    product_name,
    COUNT(*) AS purchase_count,
    SUM(price) AS total_revenue,
    AVG(price) AS avg_price
FROM events
WHERE event_name = 'purchase_complete'
GROUP BY product_name
ORDER BY total_revenue DESC
LIMIT 10;
```

---

### 8. ë¯¸ì…˜ 6: BigQueryë¡œ ê³ ê¸‰ ë¶„ì„

#### ëª©í‘œ
BigQueryì— ë°ì´í„°ë¥¼ ì˜¬ë¦¬ê³  ì½”í˜¸íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

#### ë‹¨ê³„

**1) ë°ì´í„° ì—…ë¡œë“œ:**
```python
from google.cloud import bigquery
import pandas as pd

client = bigquery.Client(project='tracking-analytics')

# ë°ì´í„° ì½ê¸°
df = pd.read_csv('data/events.csv')
df['timestamp'] = pd.to_datetime(df['timestamp'])

# BigQueryì— ì—…ë¡œë“œ
table_id = 'tracking-analytics.tracking_data.events'
job = client.load_table_from_dataframe(df, table_id)
job.result()

print(f"{len(df)} í–‰ ì—…ë¡œë“œ ì™„ë£Œ")
```

**2) ì½”í˜¸íŠ¸ ë¦¬í…ì…˜ ë¶„ì„:**
```sql
-- ì£¼ë³„ ì½”í˜¸íŠ¸ ë¦¬í…ì…˜
WITH user_cohorts AS (
  SELECT
    user_id,
    DATE_TRUNC(MIN(DATE(timestamp)), WEEK) AS cohort_week
  FROM `tracking-analytics.tracking_data.events`
  GROUP BY user_id
),
user_activity AS (
  SELECT
    uc.user_id,
    uc.cohort_week,
    DATE_TRUNC(DATE(e.timestamp), WEEK) AS activity_week
  FROM user_cohorts uc
  JOIN `tracking-analytics.tracking_data.events` e
    ON uc.user_id = e.user_id
  GROUP BY uc.user_id, uc.cohort_week, activity_week
)
SELECT
  cohort_week,
  DATE_DIFF(activity_week, cohort_week, WEEK) AS weeks_since_cohort,
  COUNT(DISTINCT user_id) AS active_users
FROM user_activity
GROUP BY cohort_week, weeks_since_cohort
ORDER BY cohort_week, weeks_since_cohort;
```

---

### 9. ìµœì¢… í”„ë¡œì íŠ¸: ê²½ì˜ì§„ ë³´ê³ ì„œ ì‘ì„±

#### ëª©í‘œ
ì§€ê¸ˆê¹Œì§€ì˜ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ê²½ì˜ì§„ì—ê²Œ ì œì¶œí•  ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

#### ë³´ê³ ì„œ êµ¬ì¡°

```markdown
# ShopKorea 1ì›” ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ

## 1. Executive Summary
- DAU: XXXëª…
- ì´ ë§¤ì¶œ: XXXì›
- ì „í™˜ìœ¨: X.X%

## 2. ì£¼ìš” ë°œê²¬ì‚¬í•­
### 2.1 í¼ë„ ë¶„ì„
- ê°€ì¥ í° ì´íƒˆ ì§€ì : ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸° (55.85% ì´íƒˆ)
- ê°œì„  í•„ìš”

### 2.2 ê³ ê°€ì¹˜ ê³ ê°
- Champions ì„¸ê·¸ë¨¼íŠ¸: 12ëª…
- í‰ê·  êµ¬ë§¤ì•¡: 458,333ì›

### 2.3 í”Œë«í¼ë³„ ì„±ê³¼
- iOS: ì „í™˜ìœ¨ ê°€ì¥ ë†’ìŒ (XX%)
- Web: ê°œì„  í•„ìš”

## 3. ì•¡ì…˜ ì•„ì´í…œ
1. ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸° ìœ ë„ CTA ê°œì„ 
2. iOS ì‚¬ìš©ì ëŒ€ìƒ í”„ë¦¬ë¯¸ì—„ ìƒí’ˆ ë§ˆì¼€íŒ…
3. Web UX ê°œì„  í”„ë¡œì íŠ¸ ì‹œì‘
4. ì €ë… ì‹œê°„ëŒ€ íƒ€ê²Ÿ í‘¸ì‹œ ì•Œë¦¼

## 4. ë‹¤ìŒ ë‹¬ ëª©í‘œ
- ì „í™˜ìœ¨ 15% ë‹¬ì„±
- Champions ê³ ê° 20ëª… í™•ë³´
```

---

### 10. ì¶”ê°€ ë„ì „ ê³¼ì œ

#### ë„ì „ 1: ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ êµ¬ë§¤ ì˜ˆì¸¡

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# ì‚¬ìš©ìë³„ íŠ¹ì„± ì¶”ì¶œ
user_features = df.groupby('user_id').agg({
    'event_name': 'count',  # ì´ ì´ë²¤íŠ¸ ìˆ˜
    'product_view': lambda x: (x == 'product_view').sum(),
    'add_to_cart': lambda x: (x == 'add_to_cart').sum(),
    'platform': lambda x: x.mode()[0] if len(x) > 0 else 'Unknown'
})

# êµ¬ë§¤ ì—¬ë¶€ (íƒ€ê²Ÿ)
user_features['purchased'] = df[df['event_name'] == 'purchase_complete']['user_id'].unique()
user_features['purchased'] = user_features.index.isin(user_features['purchased']).astype(int)

# ëª¨ë¸ í•™ìŠµ
X = user_features.drop('purchased', axis=1)
y = user_features['purchased']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# í‰ê°€
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
```

#### ë„ì „ 2: ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

Streamlitìœ¼ë¡œ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ë§Œë“¤ê¸°:

```python
import streamlit as st
import pandas as pd
import plotly.express as px

st.title("ShopKorea ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ")

# ë°ì´í„° ë¡œë“œ
@st.cache_data
def load_data():
    return pd.read_parquet('data/events.parquet')

df = load_data()

# KPI
col1, col2, col3 = st.columns(3)
col1.metric("DAU", df['user_id'].nunique())
col2.metric("ì´ ì´ë²¤íŠ¸", len(df))
col3.metric("êµ¬ë§¤ ê±´ìˆ˜", len(df[df['event_name'] == 'purchase_complete']))

# ì¼ë³„ íŠ¸ë Œë“œ
daily = df.groupby(df['timestamp'].dt.date).size()
fig = px.line(x=daily.index, y=daily.values, title="ì¼ë³„ ì´ë²¤íŠ¸ ì¶”ì´")
st.plotly_chart(fig)
```

**ì‹¤í–‰:**
```bash
streamlit run dashboard.py
```

---